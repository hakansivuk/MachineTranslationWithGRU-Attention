{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineTranslationWithGRU&Attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlRM_6Upi37-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMuMAIA88gIy",
        "colab_type": "text"
      },
      "source": [
        "Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWaLUbrTjEJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "from string import digits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY--kvpH8h6s",
        "colab_type": "text"
      },
      "source": [
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx1CnE2ajHaW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c0110a2f-7316-41d3-de34-57a312b928be"
      },
      "source": [
        "#Read the data\n",
        "data_path = \"/content/gdrive/My Drive/NLP/tur.txt\"\n",
        "lines_raw= pd.read_table(data_path,names=['source', 'target', 'a'])\n",
        "lines_raw.sample(5)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "      <th>a</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>634</th>\n",
              "      <td>I'm a nurse.</td>\n",
              "      <td>Hemşireyim.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125381</th>\n",
              "      <td>Many sects have initiation rituals for new mem...</td>\n",
              "      <td>Birçok tarikatın yeni üyeler için başlangıç ​​...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113257</th>\n",
              "      <td>Tom moved closer to see what was going on.</td>\n",
              "      <td>Tom neler olduğunu görebilmek için yanaştı.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41339</th>\n",
              "      <td>The world has five oceans.</td>\n",
              "      <td>Dünyada beş tane okyanus vardır.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53367</th>\n",
              "      <td>My brother is older than me.</td>\n",
              "      <td>Erkek kardeşim benden büyüktür.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   source  ...                                                  a\n",
              "634                                          I'm a nurse.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "125381  Many sects have initiation rituals for new mem...  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #1...\n",
              "113257         Tom moved closer to see what was going on.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #7...\n",
              "41339                          The world has five oceans.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n",
              "53367                        My brother is older than me.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq2tV3rf8kIf",
        "colab_type": "text"
      },
      "source": [
        "Prepocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC50Kqr-jHxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    #sentence = unicode_to_ascii(sentence.lower().strip())\n",
        "    num_digits= str.maketrans('','', digits)\n",
        "    \n",
        "    sentence= sentence.lower()\n",
        "    sentence= re.sub(\" +\", \" \", sentence)\n",
        "    sentence= re.sub(\"'\", '', sentence)\n",
        "    sentence= re.sub(\"ç\", \"c\", sentence)\n",
        "    sentence= re.sub(\"ö\", \"o\", sentence)\n",
        "    sentence= re.sub(\"ı\", \"i\", sentence)\n",
        "    sentence= re.sub(\"ğ\", \"g\", sentence)\n",
        "    sentence= re.sub(\"ü\", \"u\", sentence)\n",
        "    sentence= re.sub(\"ş\", \"s\", sentence)\n",
        "    sentence= sentence.translate(num_digits)\n",
        "    sentence= sentence.strip()\n",
        "    sentence= re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.rstrip().strip()\n",
        "    sentence=  'start_ ' + sentence + ' _end'\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZBOlQkOjNU8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c83c82c4-09e4-42b0-c188-422b4eaa71a9"
      },
      "source": [
        "sentence = u\"Tom geçen pazartesi Boston'a geldi.\"\n",
        "print(preprocess_sentence(sentence).encode('utf-8'))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'start_ tom gecen pazartesi bostona geldi . _end'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAqlZETWjOsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  x = lines[0].split('\\t')[0:2]\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[0:2]]  for l in lines[:num_examples]]\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM9VvGzGjP2E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "87b3e7b4-6fd2-4f5d-ac00-fef584622cfb"
      },
      "source": [
        "sample_size=60000\n",
        "source, target = create_dataset(data_path, sample_size)\n",
        "print(source[-1])\n",
        "print(target[-1])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start_ security was extremely tight . _end\n",
            "start_ guvenlik son derece sikiydi . _end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOYOVxeljQ9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuttemsvjTkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "source_sentence_tokenizer.fit_on_texts(source)\n",
        "source_tensor = source_sentence_tokenizer.texts_to_sequences(source)\n",
        "source_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_tensor,padding='post' )\n",
        "\n",
        "target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "target_sentence_tokenizer.fit_on_texts(target)\n",
        "target_tensor = target_sentence_tokenizer.texts_to_sequences(target)\n",
        "target_tensor= tf.keras.preprocessing.sequence.pad_sequences(target_tensor,padding='post' )\n",
        "\n",
        "max_target_length= max(len(t) for t in  target_tensor)\n",
        "max_source_length= max(len(t) for t in  source_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAPhYOz9j6NQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31a9a8f5-cff0-42c2-da77-4d9e38481d47"
      },
      "source": [
        "source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor= train_test_split(source_tensor, target_tensor,test_size=0.2)\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(source_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48000 48000 12000 12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSyUv3C_j7Vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1elsdhUj80U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "1436bbe0-7dd3-4d6d-9502-215ba99d1ab5"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(source_sentence_tokenizer, source_train_tensor[0])\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert( target_sentence_tokenizer, target_train_tensor[0])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> start_\n",
            "5 ----> i\n",
            "8625 ----> struggled\n",
            "31 ----> for\n",
            "9 ----> a\n",
            "443 ----> few\n",
            "1419 ----> months\n",
            "3 ----> .\n",
            "2 ----> _end\n",
            "Target Language; index to word mapping\n",
            "1 ----> start_\n",
            "262 ----> birkac\n",
            "730 ----> ay\n",
            "542 ----> boyunca\n",
            "11665 ----> ugrastim\n",
            "3 ----> .\n",
            "2 ----> _end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urugIVWDj__T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(source_train_tensor)\n",
        "BATCH_SIZE = 512\n",
        "steps_per_epoch = len(source_train_tensor)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(source_sentence_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(target_sentence_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA9QqGkEkBIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "969b2312-398a-463c-9d27-c12dddff906a"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([512, 12]), TensorShape([512, 14]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qVfii8FkCfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiPDs9RZkENW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9ba1a04c-9c83-41ef-ab31-916ee19e067c"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (512, 12, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (512, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBDiHM4DkFXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TANcNgOCkGcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "72aec9e0-7d12-4cdd-c285-16febfaa5741"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (512, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (512, 12, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVc1VCowkHuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsORO71jkIwa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7c4367c-b2dd-4b7a-bf87-473f68bf8818"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (512, 27133)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEqR4J92kKDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGmFLtQDkK_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = 'training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Zi7es9kMEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNIIY5vmkNIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6dee4b5b-b9a6-419c-ed26-72ade949fc8a"
      },
      "source": [
        "EPOCHS = 25\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    #if batch % BATCH_SIZE == 0:\n",
        "    print('Epoch {} Batch {} loss {}'.format(epoch + 1,batch, batch_loss.numpy()))\n",
        "   \n",
        "      \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 loss 2.603343963623047\n",
            "Epoch 1 Batch 1 loss 2.5937812328338623\n",
            "Epoch 1 Batch 2 loss 2.5732998847961426\n",
            "Epoch 1 Batch 3 loss 2.5598866939544678\n",
            "Epoch 1 Batch 4 loss 2.592618465423584\n",
            "Epoch 1 Batch 5 loss 2.497288465499878\n",
            "Epoch 1 Batch 6 loss 2.5177695751190186\n",
            "Epoch 1 Batch 7 loss 2.5041568279266357\n",
            "Epoch 1 Batch 8 loss 2.5405240058898926\n",
            "Epoch 1 Batch 9 loss 2.5173771381378174\n",
            "Epoch 1 Batch 10 loss 2.515714168548584\n",
            "Epoch 1 Batch 11 loss 2.4557788372039795\n",
            "Epoch 1 Batch 12 loss 2.497453212738037\n",
            "Epoch 1 Batch 13 loss 2.487307548522949\n",
            "Epoch 1 Batch 14 loss 2.4712512493133545\n",
            "Epoch 1 Batch 15 loss 2.495346784591675\n",
            "Epoch 1 Batch 16 loss 2.5118398666381836\n",
            "Epoch 1 Batch 17 loss 2.451035499572754\n",
            "Epoch 1 Batch 18 loss 2.466735363006592\n",
            "Epoch 1 Batch 19 loss 2.433310031890869\n",
            "Epoch 1 Batch 20 loss 2.4613921642303467\n",
            "Epoch 1 Batch 21 loss 2.4225175380706787\n",
            "Epoch 1 Batch 22 loss 2.4479880332946777\n",
            "Epoch 1 Batch 23 loss 2.460169553756714\n",
            "Epoch 1 Batch 24 loss 2.429664373397827\n",
            "Epoch 1 Batch 25 loss 2.4120516777038574\n",
            "Epoch 1 Batch 26 loss 2.4088170528411865\n",
            "Epoch 1 Batch 27 loss 2.413397789001465\n",
            "Epoch 1 Batch 28 loss 2.4643356800079346\n",
            "Epoch 1 Batch 29 loss 2.4009807109832764\n",
            "Epoch 1 Batch 30 loss 2.3609330654144287\n",
            "Epoch 1 Batch 31 loss 2.4278273582458496\n",
            "Epoch 1 Batch 32 loss 2.45843768119812\n",
            "Epoch 1 Batch 33 loss 2.329843044281006\n",
            "Epoch 1 Batch 34 loss 2.362034320831299\n",
            "Epoch 1 Batch 35 loss 2.389188289642334\n",
            "Epoch 1 Batch 36 loss 2.3753297328948975\n",
            "Epoch 1 Batch 37 loss 2.3572616577148438\n",
            "Epoch 1 Batch 38 loss 2.3554866313934326\n",
            "Epoch 1 Batch 39 loss 2.3267605304718018\n",
            "Epoch 1 Batch 40 loss 2.3613924980163574\n",
            "Epoch 1 Batch 41 loss 2.3332302570343018\n",
            "Epoch 1 Batch 42 loss 2.281322956085205\n",
            "Epoch 1 Batch 43 loss 2.2852859497070312\n",
            "Epoch 1 Batch 44 loss 2.2794599533081055\n",
            "Epoch 1 Batch 45 loss 2.334120273590088\n",
            "Epoch 1 Batch 46 loss 2.3394670486450195\n",
            "Epoch 1 Batch 47 loss 2.293713092803955\n",
            "Epoch 1 Batch 48 loss 2.3122055530548096\n",
            "Epoch 1 Batch 49 loss 2.311371326446533\n",
            "Epoch 1 Batch 50 loss 2.2529191970825195\n",
            "Epoch 1 Batch 51 loss 2.277674674987793\n",
            "Epoch 1 Batch 52 loss 2.2589621543884277\n",
            "Epoch 1 Batch 53 loss 2.2578623294830322\n",
            "Epoch 1 Batch 54 loss 2.227661609649658\n",
            "Epoch 1 Batch 55 loss 2.2700321674346924\n",
            "Epoch 1 Batch 56 loss 2.259735107421875\n",
            "Epoch 1 Batch 57 loss 2.254488229751587\n",
            "Epoch 1 Batch 58 loss 2.267866849899292\n",
            "Epoch 1 Batch 59 loss 2.2372360229492188\n",
            "Epoch 1 Batch 60 loss 2.191850423812866\n",
            "Epoch 1 Batch 61 loss 2.259813070297241\n",
            "Epoch 1 Batch 62 loss 2.2142043113708496\n",
            "Epoch 1 Batch 63 loss 2.1954898834228516\n",
            "Epoch 1 Batch 64 loss 2.2131612300872803\n",
            "Epoch 1 Batch 65 loss 2.211564779281616\n",
            "Epoch 1 Batch 66 loss 2.2247962951660156\n",
            "Epoch 1 Batch 67 loss 2.2083051204681396\n",
            "Epoch 1 Batch 68 loss 2.250392198562622\n",
            "Epoch 1 Batch 69 loss 2.246155261993408\n",
            "Epoch 1 Batch 70 loss 2.2477641105651855\n",
            "Epoch 1 Batch 71 loss 2.2346482276916504\n",
            "Epoch 1 Batch 72 loss 2.2203986644744873\n",
            "Epoch 1 Batch 73 loss 2.236616611480713\n",
            "Epoch 1 Batch 74 loss 2.2084286212921143\n",
            "Epoch 1 Batch 75 loss 2.1767616271972656\n",
            "Epoch 1 Batch 76 loss 2.249889373779297\n",
            "Epoch 1 Batch 77 loss 2.2304420471191406\n",
            "Epoch 1 Batch 78 loss 2.2007014751434326\n",
            "Epoch 1 Batch 79 loss 2.1799798011779785\n",
            "Epoch 1 Batch 80 loss 2.21175479888916\n",
            "Epoch 1 Batch 81 loss 2.222365140914917\n",
            "Epoch 1 Batch 82 loss 2.183960199356079\n",
            "Epoch 1 Batch 83 loss 2.158535957336426\n",
            "Epoch 1 Batch 84 loss 2.178053855895996\n",
            "Epoch 1 Batch 85 loss 2.2166335582733154\n",
            "Epoch 1 Batch 86 loss 2.1766409873962402\n",
            "Epoch 1 Batch 87 loss 2.188277006149292\n",
            "Epoch 1 Batch 88 loss 2.164444923400879\n",
            "Epoch 1 Batch 89 loss 2.200514793395996\n",
            "Epoch 1 Batch 90 loss 2.1754231452941895\n",
            "Epoch 1 Batch 91 loss 2.1929244995117188\n",
            "Epoch 1 Batch 92 loss 2.1997289657592773\n",
            "Epoch 1 Loss 2.3324\n",
            "Time taken for 1 epoch 121.77298879623413 sec\n",
            "\n",
            "Epoch 2 Batch 0 loss 2.118134021759033\n",
            "Epoch 2 Batch 1 loss 2.123487710952759\n",
            "Epoch 2 Batch 2 loss 2.126654863357544\n",
            "Epoch 2 Batch 3 loss 2.0734565258026123\n",
            "Epoch 2 Batch 4 loss 2.094806671142578\n",
            "Epoch 2 Batch 5 loss 2.0950186252593994\n",
            "Epoch 2 Batch 6 loss 2.0843191146850586\n",
            "Epoch 2 Batch 7 loss 2.1033856868743896\n",
            "Epoch 2 Batch 8 loss 2.105181932449341\n",
            "Epoch 2 Batch 9 loss 2.1139919757843018\n",
            "Epoch 2 Batch 10 loss 2.130972385406494\n",
            "Epoch 2 Batch 11 loss 2.06793475151062\n",
            "Epoch 2 Batch 12 loss 2.100154399871826\n",
            "Epoch 2 Batch 13 loss 2.0729079246520996\n",
            "Epoch 2 Batch 14 loss 2.086214542388916\n",
            "Epoch 2 Batch 15 loss 2.041229248046875\n",
            "Epoch 2 Batch 16 loss 2.0397000312805176\n",
            "Epoch 2 Batch 17 loss 2.1486053466796875\n",
            "Epoch 2 Batch 18 loss 2.0808839797973633\n",
            "Epoch 2 Batch 19 loss 2.1126866340637207\n",
            "Epoch 2 Batch 20 loss 2.1092476844787598\n",
            "Epoch 2 Batch 21 loss 2.105895757675171\n",
            "Epoch 2 Batch 22 loss 2.0876145362854004\n",
            "Epoch 2 Batch 23 loss 2.109600067138672\n",
            "Epoch 2 Batch 24 loss 2.099808931350708\n",
            "Epoch 2 Batch 25 loss 2.0602216720581055\n",
            "Epoch 2 Batch 26 loss 2.044729709625244\n",
            "Epoch 2 Batch 27 loss 2.089118480682373\n",
            "Epoch 2 Batch 28 loss 2.060265064239502\n",
            "Epoch 2 Batch 29 loss 2.088590621948242\n",
            "Epoch 2 Batch 30 loss 2.1271750926971436\n",
            "Epoch 2 Batch 31 loss 2.076322317123413\n",
            "Epoch 2 Batch 32 loss 2.0354838371276855\n",
            "Epoch 2 Batch 33 loss 2.0742123126983643\n",
            "Epoch 2 Batch 34 loss 2.070000410079956\n",
            "Epoch 2 Batch 35 loss 2.091310501098633\n",
            "Epoch 2 Batch 36 loss 2.078864097595215\n",
            "Epoch 2 Batch 37 loss 2.069868803024292\n",
            "Epoch 2 Batch 38 loss 2.11141300201416\n",
            "Epoch 2 Batch 39 loss 2.031700849533081\n",
            "Epoch 2 Batch 40 loss 2.0473368167877197\n",
            "Epoch 2 Batch 41 loss 2.0656614303588867\n",
            "Epoch 2 Batch 42 loss 2.0273189544677734\n",
            "Epoch 2 Batch 43 loss 2.0201354026794434\n",
            "Epoch 2 Batch 44 loss 2.0323305130004883\n",
            "Epoch 2 Batch 45 loss 2.0665647983551025\n",
            "Epoch 2 Batch 46 loss 2.113713502883911\n",
            "Epoch 2 Batch 47 loss 2.0358757972717285\n",
            "Epoch 2 Batch 48 loss 2.040790319442749\n",
            "Epoch 2 Batch 49 loss 2.06237530708313\n",
            "Epoch 2 Batch 50 loss 2.0830962657928467\n",
            "Epoch 2 Batch 51 loss 2.0593159198760986\n",
            "Epoch 2 Batch 52 loss 2.0348753929138184\n",
            "Epoch 2 Batch 53 loss 2.046534776687622\n",
            "Epoch 2 Batch 54 loss 2.0758016109466553\n",
            "Epoch 2 Batch 55 loss 2.0395944118499756\n",
            "Epoch 2 Batch 56 loss 2.0812957286834717\n",
            "Epoch 2 Batch 57 loss 2.0287771224975586\n",
            "Epoch 2 Batch 58 loss 2.0316548347473145\n",
            "Epoch 2 Batch 59 loss 2.0547478199005127\n",
            "Epoch 2 Batch 60 loss 2.0584895610809326\n",
            "Epoch 2 Batch 61 loss 2.06646990776062\n",
            "Epoch 2 Batch 62 loss 2.043949842453003\n",
            "Epoch 2 Batch 63 loss 2.049727201461792\n",
            "Epoch 2 Batch 64 loss 2.075716495513916\n",
            "Epoch 2 Batch 65 loss 2.0578019618988037\n",
            "Epoch 2 Batch 66 loss 2.0358800888061523\n",
            "Epoch 2 Batch 67 loss 2.059314012527466\n",
            "Epoch 2 Batch 68 loss 2.0686957836151123\n",
            "Epoch 2 Batch 69 loss 2.035968542098999\n",
            "Epoch 2 Batch 70 loss 2.046375274658203\n",
            "Epoch 2 Batch 71 loss 2.0487072467803955\n",
            "Epoch 2 Batch 72 loss 2.0491228103637695\n",
            "Epoch 2 Batch 73 loss 2.0790839195251465\n",
            "Epoch 2 Batch 74 loss 2.030721426010132\n",
            "Epoch 2 Batch 75 loss 2.0197458267211914\n",
            "Epoch 2 Batch 76 loss 2.0372467041015625\n",
            "Epoch 2 Batch 77 loss 2.050076961517334\n",
            "Epoch 2 Batch 78 loss 2.043065309524536\n",
            "Epoch 2 Batch 79 loss 2.0857462882995605\n",
            "Epoch 2 Batch 80 loss 2.056164503097534\n",
            "Epoch 2 Batch 81 loss 2.0363125801086426\n",
            "Epoch 2 Batch 82 loss 2.0454583168029785\n",
            "Epoch 2 Batch 83 loss 2.0381524562835693\n",
            "Epoch 2 Batch 84 loss 2.0113439559936523\n",
            "Epoch 2 Batch 85 loss 1.97227144241333\n",
            "Epoch 2 Batch 86 loss 2.057774066925049\n",
            "Epoch 2 Batch 87 loss 1.9793272018432617\n",
            "Epoch 2 Batch 88 loss 2.004580497741699\n",
            "Epoch 2 Batch 89 loss 2.0597662925720215\n",
            "Epoch 2 Batch 90 loss 2.0674004554748535\n",
            "Epoch 2 Batch 91 loss 1.9457662105560303\n",
            "Epoch 2 Batch 92 loss 2.010878562927246\n",
            "Epoch 2 Loss 2.0636\n",
            "Time taken for 1 epoch 123.93182682991028 sec\n",
            "\n",
            "Epoch 3 Batch 0 loss 1.9908332824707031\n",
            "Epoch 3 Batch 1 loss 1.9615737199783325\n",
            "Epoch 3 Batch 2 loss 1.9550069570541382\n",
            "Epoch 3 Batch 3 loss 1.9630217552185059\n",
            "Epoch 3 Batch 4 loss 2.0001447200775146\n",
            "Epoch 3 Batch 5 loss 1.9414228200912476\n",
            "Epoch 3 Batch 6 loss 2.0013773441314697\n",
            "Epoch 3 Batch 7 loss 2.0265321731567383\n",
            "Epoch 3 Batch 8 loss 1.9651145935058594\n",
            "Epoch 3 Batch 9 loss 1.940019130706787\n",
            "Epoch 3 Batch 10 loss 1.9398462772369385\n",
            "Epoch 3 Batch 11 loss 1.9706326723098755\n",
            "Epoch 3 Batch 12 loss 1.9774326086044312\n",
            "Epoch 3 Batch 13 loss 1.9862899780273438\n",
            "Epoch 3 Batch 14 loss 1.972827672958374\n",
            "Epoch 3 Batch 15 loss 1.9859869480133057\n",
            "Epoch 3 Batch 16 loss 1.9513800144195557\n",
            "Epoch 3 Batch 17 loss 1.9626108407974243\n",
            "Epoch 3 Batch 18 loss 1.9586395025253296\n",
            "Epoch 3 Batch 19 loss 1.9814417362213135\n",
            "Epoch 3 Batch 20 loss 1.9368174076080322\n",
            "Epoch 3 Batch 21 loss 1.9834665060043335\n",
            "Epoch 3 Batch 22 loss 1.9625102281570435\n",
            "Epoch 3 Batch 23 loss 1.9493725299835205\n",
            "Epoch 3 Batch 24 loss 1.979452133178711\n",
            "Epoch 3 Batch 25 loss 1.9488970041275024\n",
            "Epoch 3 Batch 26 loss 1.9580821990966797\n",
            "Epoch 3 Batch 27 loss 1.989683747291565\n",
            "Epoch 3 Batch 28 loss 1.9634571075439453\n",
            "Epoch 3 Batch 29 loss 1.973881721496582\n",
            "Epoch 3 Batch 30 loss 1.9824916124343872\n",
            "Epoch 3 Batch 31 loss 1.9824252128601074\n",
            "Epoch 3 Batch 32 loss 1.9696686267852783\n",
            "Epoch 3 Batch 33 loss 1.9732533693313599\n",
            "Epoch 3 Batch 34 loss 1.9833852052688599\n",
            "Epoch 3 Batch 35 loss 1.9692872762680054\n",
            "Epoch 3 Batch 36 loss 1.9752330780029297\n",
            "Epoch 3 Batch 37 loss 2.010892391204834\n",
            "Epoch 3 Batch 38 loss 1.927851676940918\n",
            "Epoch 3 Batch 39 loss 1.975324034690857\n",
            "Epoch 3 Batch 40 loss 1.971245527267456\n",
            "Epoch 3 Batch 41 loss 2.002882957458496\n",
            "Epoch 3 Batch 42 loss 1.9462271928787231\n",
            "Epoch 3 Batch 43 loss 1.9678144454956055\n",
            "Epoch 3 Batch 44 loss 1.9991718530654907\n",
            "Epoch 3 Batch 45 loss 1.9625288248062134\n",
            "Epoch 3 Batch 46 loss 1.9178335666656494\n",
            "Epoch 3 Batch 47 loss 1.969465732574463\n",
            "Epoch 3 Batch 48 loss 1.9263554811477661\n",
            "Epoch 3 Batch 49 loss 1.9230399131774902\n",
            "Epoch 3 Batch 50 loss 1.9857476949691772\n",
            "Epoch 3 Batch 51 loss 1.9384371042251587\n",
            "Epoch 3 Batch 52 loss 1.9753564596176147\n",
            "Epoch 3 Batch 53 loss 1.9153988361358643\n",
            "Epoch 3 Batch 54 loss 1.9469032287597656\n",
            "Epoch 3 Batch 55 loss 1.914556860923767\n",
            "Epoch 3 Batch 56 loss 1.9735223054885864\n",
            "Epoch 3 Batch 57 loss 1.9727040529251099\n",
            "Epoch 3 Batch 58 loss 1.8986917734146118\n",
            "Epoch 3 Batch 59 loss 1.9588102102279663\n",
            "Epoch 3 Batch 60 loss 1.959267258644104\n",
            "Epoch 3 Batch 61 loss 1.963000774383545\n",
            "Epoch 3 Batch 62 loss 1.9184967279434204\n",
            "Epoch 3 Batch 63 loss 1.9607092142105103\n",
            "Epoch 3 Batch 64 loss 1.923408031463623\n",
            "Epoch 3 Batch 65 loss 1.9066745042800903\n",
            "Epoch 3 Batch 66 loss 1.9652526378631592\n",
            "Epoch 3 Batch 67 loss 1.961050271987915\n",
            "Epoch 3 Batch 68 loss 1.9476511478424072\n",
            "Epoch 3 Batch 69 loss 1.9448786973953247\n",
            "Epoch 3 Batch 70 loss 1.9937942028045654\n",
            "Epoch 3 Batch 71 loss 1.9095863103866577\n",
            "Epoch 3 Batch 72 loss 1.9498780965805054\n",
            "Epoch 3 Batch 73 loss 1.9534878730773926\n",
            "Epoch 3 Batch 74 loss 1.8999608755111694\n",
            "Epoch 3 Batch 75 loss 1.9420013427734375\n",
            "Epoch 3 Batch 76 loss 1.9001591205596924\n",
            "Epoch 3 Batch 77 loss 1.911659836769104\n",
            "Epoch 3 Batch 78 loss 1.9643479585647583\n",
            "Epoch 3 Batch 79 loss 1.9485790729522705\n",
            "Epoch 3 Batch 80 loss 1.943332314491272\n",
            "Epoch 3 Batch 81 loss 1.89939284324646\n",
            "Epoch 3 Batch 82 loss 2.024822950363159\n",
            "Epoch 3 Batch 83 loss 1.8925671577453613\n",
            "Epoch 3 Batch 84 loss 1.933701753616333\n",
            "Epoch 3 Batch 85 loss 1.8866517543792725\n",
            "Epoch 3 Batch 86 loss 1.9288069009780884\n",
            "Epoch 3 Batch 87 loss 1.9411349296569824\n",
            "Epoch 3 Batch 88 loss 1.9447352886199951\n",
            "Epoch 3 Batch 89 loss 1.9544439315795898\n",
            "Epoch 3 Batch 90 loss 1.908887267112732\n",
            "Epoch 3 Batch 91 loss 1.9124855995178223\n",
            "Epoch 3 Batch 92 loss 1.9182806015014648\n",
            "Epoch 3 Loss 1.9549\n",
            "Time taken for 1 epoch 121.93322968482971 sec\n",
            "\n",
            "Epoch 4 Batch 0 loss 1.907286286354065\n",
            "Epoch 4 Batch 1 loss 1.904640555381775\n",
            "Epoch 4 Batch 2 loss 1.8930342197418213\n",
            "Epoch 4 Batch 3 loss 1.908908486366272\n",
            "Epoch 4 Batch 4 loss 1.878674864768982\n",
            "Epoch 4 Batch 5 loss 1.8790147304534912\n",
            "Epoch 4 Batch 6 loss 1.9026274681091309\n",
            "Epoch 4 Batch 7 loss 1.855946660041809\n",
            "Epoch 4 Batch 8 loss 1.8671785593032837\n",
            "Epoch 4 Batch 9 loss 1.8457473516464233\n",
            "Epoch 4 Batch 10 loss 1.8627533912658691\n",
            "Epoch 4 Batch 11 loss 1.8962171077728271\n",
            "Epoch 4 Batch 12 loss 1.8927727937698364\n",
            "Epoch 4 Batch 13 loss 1.8540160655975342\n",
            "Epoch 4 Batch 14 loss 1.8949458599090576\n",
            "Epoch 4 Batch 15 loss 1.8210387229919434\n",
            "Epoch 4 Batch 16 loss 1.8525216579437256\n",
            "Epoch 4 Batch 17 loss 1.8650422096252441\n",
            "Epoch 4 Batch 18 loss 1.8861275911331177\n",
            "Epoch 4 Batch 19 loss 1.8901628255844116\n",
            "Epoch 4 Batch 20 loss 1.9090700149536133\n",
            "Epoch 4 Batch 21 loss 1.895094394683838\n",
            "Epoch 4 Batch 22 loss 1.8836123943328857\n",
            "Epoch 4 Batch 23 loss 1.846297264099121\n",
            "Epoch 4 Batch 24 loss 1.876253366470337\n",
            "Epoch 4 Batch 25 loss 1.8437855243682861\n",
            "Epoch 4 Batch 26 loss 1.8482074737548828\n",
            "Epoch 4 Batch 27 loss 1.8578376770019531\n",
            "Epoch 4 Batch 28 loss 1.8654460906982422\n",
            "Epoch 4 Batch 29 loss 1.846683144569397\n",
            "Epoch 4 Batch 30 loss 1.859680414199829\n",
            "Epoch 4 Batch 31 loss 1.8952465057373047\n",
            "Epoch 4 Batch 32 loss 1.8695199489593506\n",
            "Epoch 4 Batch 33 loss 1.8554874658584595\n",
            "Epoch 4 Batch 34 loss 1.8792686462402344\n",
            "Epoch 4 Batch 35 loss 1.870099663734436\n",
            "Epoch 4 Batch 36 loss 1.88812255859375\n",
            "Epoch 4 Batch 37 loss 1.8891637325286865\n",
            "Epoch 4 Batch 38 loss 1.8584014177322388\n",
            "Epoch 4 Batch 39 loss 1.874019742012024\n",
            "Epoch 4 Batch 40 loss 1.8585002422332764\n",
            "Epoch 4 Batch 41 loss 1.8672008514404297\n",
            "Epoch 4 Batch 42 loss 1.8796770572662354\n",
            "Epoch 4 Batch 43 loss 1.869485855102539\n",
            "Epoch 4 Batch 44 loss 1.8962539434432983\n",
            "Epoch 4 Batch 45 loss 1.8712750673294067\n",
            "Epoch 4 Batch 46 loss 1.8442736864089966\n",
            "Epoch 4 Batch 47 loss 1.8491497039794922\n",
            "Epoch 4 Batch 48 loss 1.8603789806365967\n",
            "Epoch 4 Batch 49 loss 1.8547909259796143\n",
            "Epoch 4 Batch 50 loss 1.8530220985412598\n",
            "Epoch 4 Batch 51 loss 1.8607611656188965\n",
            "Epoch 4 Batch 52 loss 1.8288902044296265\n",
            "Epoch 4 Batch 53 loss 1.8745033740997314\n",
            "Epoch 4 Batch 54 loss 1.877000331878662\n",
            "Epoch 4 Batch 55 loss 1.8313531875610352\n",
            "Epoch 4 Batch 56 loss 1.84138822555542\n",
            "Epoch 4 Batch 57 loss 1.8310649394989014\n",
            "Epoch 4 Batch 58 loss 1.8282554149627686\n",
            "Epoch 4 Batch 59 loss 1.8255618810653687\n",
            "Epoch 4 Batch 60 loss 1.8213998079299927\n",
            "Epoch 4 Batch 61 loss 1.848770260810852\n",
            "Epoch 4 Batch 62 loss 1.8713455200195312\n",
            "Epoch 4 Batch 63 loss 1.8845796585083008\n",
            "Epoch 4 Batch 64 loss 1.8486653566360474\n",
            "Epoch 4 Batch 65 loss 1.8363347053527832\n",
            "Epoch 4 Batch 66 loss 1.8505233526229858\n",
            "Epoch 4 Batch 67 loss 1.8395339250564575\n",
            "Epoch 4 Batch 68 loss 1.8292016983032227\n",
            "Epoch 4 Batch 69 loss 1.8103429079055786\n",
            "Epoch 4 Batch 70 loss 1.8776094913482666\n",
            "Epoch 4 Batch 71 loss 1.855633020401001\n",
            "Epoch 4 Batch 72 loss 1.876940131187439\n",
            "Epoch 4 Batch 73 loss 1.8360915184020996\n",
            "Epoch 4 Batch 74 loss 1.8885784149169922\n",
            "Epoch 4 Batch 75 loss 1.880502462387085\n",
            "Epoch 4 Batch 76 loss 1.8827234506607056\n",
            "Epoch 4 Batch 77 loss 1.870174765586853\n",
            "Epoch 4 Batch 78 loss 1.848651647567749\n",
            "Epoch 4 Batch 79 loss 1.8641020059585571\n",
            "Epoch 4 Batch 80 loss 1.798863172531128\n",
            "Epoch 4 Batch 81 loss 1.8218281269073486\n",
            "Epoch 4 Batch 82 loss 1.821200966835022\n",
            "Epoch 4 Batch 83 loss 1.8748722076416016\n",
            "Epoch 4 Batch 84 loss 1.8161060810089111\n",
            "Epoch 4 Batch 85 loss 1.8032691478729248\n",
            "Epoch 4 Batch 86 loss 1.828322172164917\n",
            "Epoch 4 Batch 87 loss 1.838233232498169\n",
            "Epoch 4 Batch 88 loss 1.8296387195587158\n",
            "Epoch 4 Batch 89 loss 1.8250690698623657\n",
            "Epoch 4 Batch 90 loss 1.840108036994934\n",
            "Epoch 4 Batch 91 loss 1.7936947345733643\n",
            "Epoch 4 Batch 92 loss 1.8300459384918213\n",
            "Epoch 4 Loss 1.8593\n",
            "Time taken for 1 epoch 123.9532618522644 sec\n",
            "\n",
            "Epoch 5 Batch 0 loss 1.767993688583374\n",
            "Epoch 5 Batch 1 loss 1.821687936782837\n",
            "Epoch 5 Batch 2 loss 1.7628800868988037\n",
            "Epoch 5 Batch 3 loss 1.7875583171844482\n",
            "Epoch 5 Batch 4 loss 1.812099814414978\n",
            "Epoch 5 Batch 5 loss 1.7700228691101074\n",
            "Epoch 5 Batch 6 loss 1.7676831483840942\n",
            "Epoch 5 Batch 7 loss 1.7711992263793945\n",
            "Epoch 5 Batch 8 loss 1.7941278219223022\n",
            "Epoch 5 Batch 9 loss 1.7556376457214355\n",
            "Epoch 5 Batch 10 loss 1.7965476512908936\n",
            "Epoch 5 Batch 11 loss 1.771167278289795\n",
            "Epoch 5 Batch 12 loss 1.729793667793274\n",
            "Epoch 5 Batch 13 loss 1.8148694038391113\n",
            "Epoch 5 Batch 14 loss 1.734241008758545\n",
            "Epoch 5 Batch 15 loss 1.7721294164657593\n",
            "Epoch 5 Batch 16 loss 1.7589993476867676\n",
            "Epoch 5 Batch 17 loss 1.7716304063796997\n",
            "Epoch 5 Batch 18 loss 1.752264142036438\n",
            "Epoch 5 Batch 19 loss 1.7756993770599365\n",
            "Epoch 5 Batch 20 loss 1.7650856971740723\n",
            "Epoch 5 Batch 21 loss 1.7465687990188599\n",
            "Epoch 5 Batch 22 loss 1.7901166677474976\n",
            "Epoch 5 Batch 23 loss 1.7590842247009277\n",
            "Epoch 5 Batch 24 loss 1.7742853164672852\n",
            "Epoch 5 Batch 25 loss 1.7791944742202759\n",
            "Epoch 5 Batch 26 loss 1.7123682498931885\n",
            "Epoch 5 Batch 27 loss 1.7403106689453125\n",
            "Epoch 5 Batch 28 loss 1.7553772926330566\n",
            "Epoch 5 Batch 29 loss 1.7440423965454102\n",
            "Epoch 5 Batch 30 loss 1.6864367723464966\n",
            "Epoch 5 Batch 31 loss 1.7773656845092773\n",
            "Epoch 5 Batch 32 loss 1.7915722131729126\n",
            "Epoch 5 Batch 33 loss 1.7999407052993774\n",
            "Epoch 5 Batch 34 loss 1.7693911790847778\n",
            "Epoch 5 Batch 35 loss 1.8005995750427246\n",
            "Epoch 5 Batch 36 loss 1.7612096071243286\n",
            "Epoch 5 Batch 37 loss 1.7626774311065674\n",
            "Epoch 5 Batch 38 loss 1.7547218799591064\n",
            "Epoch 5 Batch 39 loss 1.7604173421859741\n",
            "Epoch 5 Batch 40 loss 1.7040175199508667\n",
            "Epoch 5 Batch 41 loss 1.8275182247161865\n",
            "Epoch 5 Batch 42 loss 1.7618358135223389\n",
            "Epoch 5 Batch 43 loss 1.7666676044464111\n",
            "Epoch 5 Batch 44 loss 1.7497202157974243\n",
            "Epoch 5 Batch 45 loss 1.777719259262085\n",
            "Epoch 5 Batch 46 loss 1.7488130331039429\n",
            "Epoch 5 Batch 47 loss 1.7696679830551147\n",
            "Epoch 5 Batch 48 loss 1.7385363578796387\n",
            "Epoch 5 Batch 49 loss 1.7321492433547974\n",
            "Epoch 5 Batch 50 loss 1.7547988891601562\n",
            "Epoch 5 Batch 51 loss 1.7062046527862549\n",
            "Epoch 5 Batch 52 loss 1.6823004484176636\n",
            "Epoch 5 Batch 53 loss 1.7478611469268799\n",
            "Epoch 5 Batch 54 loss 1.764552354812622\n",
            "Epoch 5 Batch 55 loss 1.7695351839065552\n",
            "Epoch 5 Batch 56 loss 1.7380181550979614\n",
            "Epoch 5 Batch 57 loss 1.725875973701477\n",
            "Epoch 5 Batch 58 loss 1.7809982299804688\n",
            "Epoch 5 Batch 59 loss 1.7470426559448242\n",
            "Epoch 5 Batch 60 loss 1.7205318212509155\n",
            "Epoch 5 Batch 61 loss 1.7472795248031616\n",
            "Epoch 5 Batch 62 loss 1.712390661239624\n",
            "Epoch 5 Batch 63 loss 1.7351446151733398\n",
            "Epoch 5 Batch 64 loss 1.73673415184021\n",
            "Epoch 5 Batch 65 loss 1.7683883905410767\n",
            "Epoch 5 Batch 66 loss 1.7302786111831665\n",
            "Epoch 5 Batch 67 loss 1.7765352725982666\n",
            "Epoch 5 Batch 68 loss 1.7504583597183228\n",
            "Epoch 5 Batch 69 loss 1.7571430206298828\n",
            "Epoch 5 Batch 70 loss 1.7548381090164185\n",
            "Epoch 5 Batch 71 loss 1.743741750717163\n",
            "Epoch 5 Batch 72 loss 1.7718806266784668\n",
            "Epoch 5 Batch 73 loss 1.703439474105835\n",
            "Epoch 5 Batch 74 loss 1.7202575206756592\n",
            "Epoch 5 Batch 75 loss 1.7910468578338623\n",
            "Epoch 5 Batch 76 loss 1.738695740699768\n",
            "Epoch 5 Batch 77 loss 1.7179086208343506\n",
            "Epoch 5 Batch 78 loss 1.7331326007843018\n",
            "Epoch 5 Batch 79 loss 1.736154317855835\n",
            "Epoch 5 Batch 80 loss 1.7328513860702515\n",
            "Epoch 5 Batch 81 loss 1.7258130311965942\n",
            "Epoch 5 Batch 82 loss 1.73148512840271\n",
            "Epoch 5 Batch 83 loss 1.7171311378479004\n",
            "Epoch 5 Batch 84 loss 1.7028850317001343\n",
            "Epoch 5 Batch 85 loss 1.7099430561065674\n",
            "Epoch 5 Batch 86 loss 1.750510811805725\n",
            "Epoch 5 Batch 87 loss 1.754497766494751\n",
            "Epoch 5 Batch 88 loss 1.734283685684204\n",
            "Epoch 5 Batch 89 loss 1.673244833946228\n",
            "Epoch 5 Batch 90 loss 1.6916254758834839\n",
            "Epoch 5 Batch 91 loss 1.7396397590637207\n",
            "Epoch 5 Batch 92 loss 1.7376792430877686\n",
            "Epoch 5 Loss 1.7523\n",
            "Time taken for 1 epoch 122.04072570800781 sec\n",
            "\n",
            "Epoch 6 Batch 0 loss 1.7034052610397339\n",
            "Epoch 6 Batch 1 loss 1.6444580554962158\n",
            "Epoch 6 Batch 2 loss 1.6242541074752808\n",
            "Epoch 6 Batch 3 loss 1.6747990846633911\n",
            "Epoch 6 Batch 4 loss 1.6570769548416138\n",
            "Epoch 6 Batch 5 loss 1.66484797000885\n",
            "Epoch 6 Batch 6 loss 1.652130365371704\n",
            "Epoch 6 Batch 7 loss 1.6495592594146729\n",
            "Epoch 6 Batch 8 loss 1.6718555688858032\n",
            "Epoch 6 Batch 9 loss 1.6653409004211426\n",
            "Epoch 6 Batch 10 loss 1.6563165187835693\n",
            "Epoch 6 Batch 11 loss 1.6827877759933472\n",
            "Epoch 6 Batch 12 loss 1.6523582935333252\n",
            "Epoch 6 Batch 13 loss 1.6325008869171143\n",
            "Epoch 6 Batch 14 loss 1.6627657413482666\n",
            "Epoch 6 Batch 15 loss 1.7043536901474\n",
            "Epoch 6 Batch 16 loss 1.6758443117141724\n",
            "Epoch 6 Batch 17 loss 1.6525734663009644\n",
            "Epoch 6 Batch 18 loss 1.6431318521499634\n",
            "Epoch 6 Batch 19 loss 1.6485416889190674\n",
            "Epoch 6 Batch 20 loss 1.683303713798523\n",
            "Epoch 6 Batch 21 loss 1.6542551517486572\n",
            "Epoch 6 Batch 22 loss 1.6840384006500244\n",
            "Epoch 6 Batch 23 loss 1.6306723356246948\n",
            "Epoch 6 Batch 24 loss 1.667784333229065\n",
            "Epoch 6 Batch 25 loss 1.66372811794281\n",
            "Epoch 6 Batch 26 loss 1.6575249433517456\n",
            "Epoch 6 Batch 27 loss 1.6223810911178589\n",
            "Epoch 6 Batch 28 loss 1.673126459121704\n",
            "Epoch 6 Batch 29 loss 1.6404244899749756\n",
            "Epoch 6 Batch 30 loss 1.657647728919983\n",
            "Epoch 6 Batch 31 loss 1.6560777425765991\n",
            "Epoch 6 Batch 32 loss 1.6520158052444458\n",
            "Epoch 6 Batch 33 loss 1.6423239707946777\n",
            "Epoch 6 Batch 34 loss 1.66461980342865\n",
            "Epoch 6 Batch 35 loss 1.6299092769622803\n",
            "Epoch 6 Batch 36 loss 1.6610056161880493\n",
            "Epoch 6 Batch 37 loss 1.6744036674499512\n",
            "Epoch 6 Batch 38 loss 1.6499608755111694\n",
            "Epoch 6 Batch 39 loss 1.6724780797958374\n",
            "Epoch 6 Batch 40 loss 1.6253758668899536\n",
            "Epoch 6 Batch 41 loss 1.6534677743911743\n",
            "Epoch 6 Batch 42 loss 1.657123327255249\n",
            "Epoch 6 Batch 43 loss 1.625052571296692\n",
            "Epoch 6 Batch 44 loss 1.6339905261993408\n",
            "Epoch 6 Batch 45 loss 1.6743063926696777\n",
            "Epoch 6 Batch 46 loss 1.6552860736846924\n",
            "Epoch 6 Batch 47 loss 1.6712912321090698\n",
            "Epoch 6 Batch 48 loss 1.6374971866607666\n",
            "Epoch 6 Batch 49 loss 1.6594821214675903\n",
            "Epoch 6 Batch 50 loss 1.6511595249176025\n",
            "Epoch 6 Batch 51 loss 1.654935359954834\n",
            "Epoch 6 Batch 52 loss 1.6593315601348877\n",
            "Epoch 6 Batch 53 loss 1.605138897895813\n",
            "Epoch 6 Batch 54 loss 1.6540964841842651\n",
            "Epoch 6 Batch 55 loss 1.6075249910354614\n",
            "Epoch 6 Batch 56 loss 1.63692307472229\n",
            "Epoch 6 Batch 57 loss 1.6346309185028076\n",
            "Epoch 6 Batch 58 loss 1.6750435829162598\n",
            "Epoch 6 Batch 59 loss 1.673926830291748\n",
            "Epoch 6 Batch 60 loss 1.6585566997528076\n",
            "Epoch 6 Batch 61 loss 1.6707501411437988\n",
            "Epoch 6 Batch 62 loss 1.6564650535583496\n",
            "Epoch 6 Batch 63 loss 1.628014087677002\n",
            "Epoch 6 Batch 64 loss 1.6665364503860474\n",
            "Epoch 6 Batch 65 loss 1.6598833799362183\n",
            "Epoch 6 Batch 66 loss 1.6707510948181152\n",
            "Epoch 6 Batch 67 loss 1.6515952348709106\n",
            "Epoch 6 Batch 68 loss 1.6578137874603271\n",
            "Epoch 6 Batch 69 loss 1.6902916431427002\n",
            "Epoch 6 Batch 70 loss 1.6289643049240112\n",
            "Epoch 6 Batch 71 loss 1.6653554439544678\n",
            "Epoch 6 Batch 72 loss 1.6530499458312988\n",
            "Epoch 6 Batch 73 loss 1.6362789869308472\n",
            "Epoch 6 Batch 74 loss 1.6377977132797241\n",
            "Epoch 6 Batch 75 loss 1.639227271080017\n",
            "Epoch 6 Batch 76 loss 1.5984336137771606\n",
            "Epoch 6 Batch 77 loss 1.6811615228652954\n",
            "Epoch 6 Batch 78 loss 1.661720871925354\n",
            "Epoch 6 Batch 79 loss 1.6268212795257568\n",
            "Epoch 6 Batch 80 loss 1.6631720066070557\n",
            "Epoch 6 Batch 81 loss 1.6145082712173462\n",
            "Epoch 6 Batch 82 loss 1.6319596767425537\n",
            "Epoch 6 Batch 83 loss 1.6141562461853027\n",
            "Epoch 6 Batch 84 loss 1.6121506690979004\n",
            "Epoch 6 Batch 85 loss 1.613996982574463\n",
            "Epoch 6 Batch 86 loss 1.6456316709518433\n",
            "Epoch 6 Batch 87 loss 1.6413853168487549\n",
            "Epoch 6 Batch 88 loss 1.6732099056243896\n",
            "Epoch 6 Batch 89 loss 1.6547266244888306\n",
            "Epoch 6 Batch 90 loss 1.6230151653289795\n",
            "Epoch 6 Batch 91 loss 1.6616989374160767\n",
            "Epoch 6 Batch 92 loss 1.6444065570831299\n",
            "Epoch 6 Loss 1.6520\n",
            "Time taken for 1 epoch 124.06567573547363 sec\n",
            "\n",
            "Epoch 7 Batch 0 loss 1.5799810886383057\n",
            "Epoch 7 Batch 1 loss 1.5674474239349365\n",
            "Epoch 7 Batch 2 loss 1.5818250179290771\n",
            "Epoch 7 Batch 3 loss 1.5599970817565918\n",
            "Epoch 7 Batch 4 loss 1.5753291845321655\n",
            "Epoch 7 Batch 5 loss 1.5563663244247437\n",
            "Epoch 7 Batch 6 loss 1.5369150638580322\n",
            "Epoch 7 Batch 7 loss 1.5769072771072388\n",
            "Epoch 7 Batch 8 loss 1.5519146919250488\n",
            "Epoch 7 Batch 9 loss 1.512437105178833\n",
            "Epoch 7 Batch 10 loss 1.5664589405059814\n",
            "Epoch 7 Batch 11 loss 1.592819094657898\n",
            "Epoch 7 Batch 12 loss 1.5498855113983154\n",
            "Epoch 7 Batch 13 loss 1.5564614534378052\n",
            "Epoch 7 Batch 14 loss 1.5615836381912231\n",
            "Epoch 7 Batch 15 loss 1.530769944190979\n",
            "Epoch 7 Batch 16 loss 1.528314471244812\n",
            "Epoch 7 Batch 17 loss 1.5779074430465698\n",
            "Epoch 7 Batch 18 loss 1.567333698272705\n",
            "Epoch 7 Batch 19 loss 1.6099961996078491\n",
            "Epoch 7 Batch 20 loss 1.529737114906311\n",
            "Epoch 7 Batch 21 loss 1.5529885292053223\n",
            "Epoch 7 Batch 22 loss 1.5532020330429077\n",
            "Epoch 7 Batch 23 loss 1.5435246229171753\n",
            "Epoch 7 Batch 24 loss 1.5584254264831543\n",
            "Epoch 7 Batch 25 loss 1.6177743673324585\n",
            "Epoch 7 Batch 26 loss 1.5825141668319702\n",
            "Epoch 7 Batch 27 loss 1.5797092914581299\n",
            "Epoch 7 Batch 28 loss 1.5225166082382202\n",
            "Epoch 7 Batch 29 loss 1.5753320455551147\n",
            "Epoch 7 Batch 30 loss 1.5903071165084839\n",
            "Epoch 7 Batch 31 loss 1.5445722341537476\n",
            "Epoch 7 Batch 32 loss 1.564837098121643\n",
            "Epoch 7 Batch 33 loss 1.5241681337356567\n",
            "Epoch 7 Batch 34 loss 1.5703301429748535\n",
            "Epoch 7 Batch 35 loss 1.5496695041656494\n",
            "Epoch 7 Batch 36 loss 1.5110338926315308\n",
            "Epoch 7 Batch 37 loss 1.564202070236206\n",
            "Epoch 7 Batch 38 loss 1.5336521863937378\n",
            "Epoch 7 Batch 39 loss 1.5612024068832397\n",
            "Epoch 7 Batch 40 loss 1.593258261680603\n",
            "Epoch 7 Batch 41 loss 1.5519464015960693\n",
            "Epoch 7 Batch 42 loss 1.590375304222107\n",
            "Epoch 7 Batch 43 loss 1.5226540565490723\n",
            "Epoch 7 Batch 44 loss 1.5157684087753296\n",
            "Epoch 7 Batch 45 loss 1.5304747819900513\n",
            "Epoch 7 Batch 46 loss 1.5782885551452637\n",
            "Epoch 7 Batch 47 loss 1.5550333261489868\n",
            "Epoch 7 Batch 48 loss 1.5413624048233032\n",
            "Epoch 7 Batch 49 loss 1.5339702367782593\n",
            "Epoch 7 Batch 50 loss 1.544443964958191\n",
            "Epoch 7 Batch 51 loss 1.563423752784729\n",
            "Epoch 7 Batch 52 loss 1.5657403469085693\n",
            "Epoch 7 Batch 53 loss 1.551015019416809\n",
            "Epoch 7 Batch 54 loss 1.5669809579849243\n",
            "Epoch 7 Batch 55 loss 1.5459916591644287\n",
            "Epoch 7 Batch 56 loss 1.577787160873413\n",
            "Epoch 7 Batch 57 loss 1.5279542207717896\n",
            "Epoch 7 Batch 58 loss 1.5811703205108643\n",
            "Epoch 7 Batch 59 loss 1.5415008068084717\n",
            "Epoch 7 Batch 60 loss 1.5678411722183228\n",
            "Epoch 7 Batch 61 loss 1.5958808660507202\n",
            "Epoch 7 Batch 62 loss 1.551927089691162\n",
            "Epoch 7 Batch 63 loss 1.554212212562561\n",
            "Epoch 7 Batch 64 loss 1.537596344947815\n",
            "Epoch 7 Batch 65 loss 1.5186406373977661\n",
            "Epoch 7 Batch 66 loss 1.5405676364898682\n",
            "Epoch 7 Batch 67 loss 1.536885142326355\n",
            "Epoch 7 Batch 68 loss 1.569903016090393\n",
            "Epoch 7 Batch 69 loss 1.5557186603546143\n",
            "Epoch 7 Batch 70 loss 1.57667875289917\n",
            "Epoch 7 Batch 71 loss 1.5337443351745605\n",
            "Epoch 7 Batch 72 loss 1.5748027563095093\n",
            "Epoch 7 Batch 73 loss 1.5523312091827393\n",
            "Epoch 7 Batch 74 loss 1.5669934749603271\n",
            "Epoch 7 Batch 75 loss 1.5472983121871948\n",
            "Epoch 7 Batch 76 loss 1.525998592376709\n",
            "Epoch 7 Batch 77 loss 1.5569206476211548\n",
            "Epoch 7 Batch 78 loss 1.5714629888534546\n",
            "Epoch 7 Batch 79 loss 1.561253309249878\n",
            "Epoch 7 Batch 80 loss 1.5556824207305908\n",
            "Epoch 7 Batch 81 loss 1.5383530855178833\n",
            "Epoch 7 Batch 82 loss 1.5603400468826294\n",
            "Epoch 7 Batch 83 loss 1.5634310245513916\n",
            "Epoch 7 Batch 84 loss 1.6016136407852173\n",
            "Epoch 7 Batch 85 loss 1.5231717824935913\n",
            "Epoch 7 Batch 86 loss 1.554385781288147\n",
            "Epoch 7 Batch 87 loss 1.5654261112213135\n",
            "Epoch 7 Batch 88 loss 1.519346833229065\n",
            "Epoch 7 Batch 89 loss 1.5549918413162231\n",
            "Epoch 7 Batch 90 loss 1.5616966485977173\n",
            "Epoch 7 Batch 91 loss 1.5500844717025757\n",
            "Epoch 7 Batch 92 loss 1.568098545074463\n",
            "Epoch 7 Loss 1.5566\n",
            "Time taken for 1 epoch 122.36660027503967 sec\n",
            "\n",
            "Epoch 8 Batch 0 loss 1.459108829498291\n",
            "Epoch 8 Batch 1 loss 1.4351742267608643\n",
            "Epoch 8 Batch 2 loss 1.4586241245269775\n",
            "Epoch 8 Batch 3 loss 1.4727919101715088\n",
            "Epoch 8 Batch 4 loss 1.4428917169570923\n",
            "Epoch 8 Batch 5 loss 1.4543546438217163\n",
            "Epoch 8 Batch 6 loss 1.475982904434204\n",
            "Epoch 8 Batch 7 loss 1.4801865816116333\n",
            "Epoch 8 Batch 8 loss 1.4567205905914307\n",
            "Epoch 8 Batch 9 loss 1.4674135446548462\n",
            "Epoch 8 Batch 10 loss 1.500534176826477\n",
            "Epoch 8 Batch 11 loss 1.4435067176818848\n",
            "Epoch 8 Batch 12 loss 1.4841313362121582\n",
            "Epoch 8 Batch 13 loss 1.4956772327423096\n",
            "Epoch 8 Batch 14 loss 1.4463976621627808\n",
            "Epoch 8 Batch 15 loss 1.45330011844635\n",
            "Epoch 8 Batch 16 loss 1.4359279870986938\n",
            "Epoch 8 Batch 17 loss 1.4817174673080444\n",
            "Epoch 8 Batch 18 loss 1.4642800092697144\n",
            "Epoch 8 Batch 19 loss 1.4859278202056885\n",
            "Epoch 8 Batch 20 loss 1.4792447090148926\n",
            "Epoch 8 Batch 21 loss 1.4594062566757202\n",
            "Epoch 8 Batch 22 loss 1.4556461572647095\n",
            "Epoch 8 Batch 23 loss 1.486100673675537\n",
            "Epoch 8 Batch 24 loss 1.4864239692687988\n",
            "Epoch 8 Batch 25 loss 1.4168050289154053\n",
            "Epoch 8 Batch 26 loss 1.4353629350662231\n",
            "Epoch 8 Batch 27 loss 1.442323923110962\n",
            "Epoch 8 Batch 28 loss 1.5036677122116089\n",
            "Epoch 8 Batch 29 loss 1.47672438621521\n",
            "Epoch 8 Batch 30 loss 1.4233901500701904\n",
            "Epoch 8 Batch 31 loss 1.5012624263763428\n",
            "Epoch 8 Batch 32 loss 1.4652093648910522\n",
            "Epoch 8 Batch 33 loss 1.4489762783050537\n",
            "Epoch 8 Batch 34 loss 1.4555143117904663\n",
            "Epoch 8 Batch 35 loss 1.4558571577072144\n",
            "Epoch 8 Batch 36 loss 1.4555953741073608\n",
            "Epoch 8 Batch 37 loss 1.494242548942566\n",
            "Epoch 8 Batch 38 loss 1.4874454736709595\n",
            "Epoch 8 Batch 39 loss 1.4768414497375488\n",
            "Epoch 8 Batch 40 loss 1.4769834280014038\n",
            "Epoch 8 Batch 41 loss 1.474502444267273\n",
            "Epoch 8 Batch 42 loss 1.439784288406372\n",
            "Epoch 8 Batch 43 loss 1.4754780530929565\n",
            "Epoch 8 Batch 44 loss 1.4571559429168701\n",
            "Epoch 8 Batch 45 loss 1.459215521812439\n",
            "Epoch 8 Batch 46 loss 1.4407389163970947\n",
            "Epoch 8 Batch 47 loss 1.4286160469055176\n",
            "Epoch 8 Batch 48 loss 1.4579795598983765\n",
            "Epoch 8 Batch 49 loss 1.4672725200653076\n",
            "Epoch 8 Batch 50 loss 1.4622591733932495\n",
            "Epoch 8 Batch 51 loss 1.465134859085083\n",
            "Epoch 8 Batch 52 loss 1.4751056432724\n",
            "Epoch 8 Batch 53 loss 1.4640599489212036\n",
            "Epoch 8 Batch 54 loss 1.4661532640457153\n",
            "Epoch 8 Batch 55 loss 1.49679696559906\n",
            "Epoch 8 Batch 56 loss 1.4715458154678345\n",
            "Epoch 8 Batch 57 loss 1.484581470489502\n",
            "Epoch 8 Batch 58 loss 1.4768376350402832\n",
            "Epoch 8 Batch 59 loss 1.4876395463943481\n",
            "Epoch 8 Batch 60 loss 1.4661768674850464\n",
            "Epoch 8 Batch 61 loss 1.4780181646347046\n",
            "Epoch 8 Batch 62 loss 1.4673200845718384\n",
            "Epoch 8 Batch 63 loss 1.4730232954025269\n",
            "Epoch 8 Batch 64 loss 1.4724924564361572\n",
            "Epoch 8 Batch 65 loss 1.4352872371673584\n",
            "Epoch 8 Batch 66 loss 1.4601638317108154\n",
            "Epoch 8 Batch 67 loss 1.4695518016815186\n",
            "Epoch 8 Batch 68 loss 1.4883395433425903\n",
            "Epoch 8 Batch 69 loss 1.4900963306427002\n",
            "Epoch 8 Batch 70 loss 1.4364567995071411\n",
            "Epoch 8 Batch 71 loss 1.433435082435608\n",
            "Epoch 8 Batch 72 loss 1.4881685972213745\n",
            "Epoch 8 Batch 73 loss 1.433566689491272\n",
            "Epoch 8 Batch 74 loss 1.4337342977523804\n",
            "Epoch 8 Batch 75 loss 1.4470441341400146\n",
            "Epoch 8 Batch 76 loss 1.4401934146881104\n",
            "Epoch 8 Batch 77 loss 1.4198100566864014\n",
            "Epoch 8 Batch 78 loss 1.4529742002487183\n",
            "Epoch 8 Batch 79 loss 1.482995629310608\n",
            "Epoch 8 Batch 80 loss 1.445844292640686\n",
            "Epoch 8 Batch 81 loss 1.4397995471954346\n",
            "Epoch 8 Batch 82 loss 1.4938056468963623\n",
            "Epoch 8 Batch 83 loss 1.4593379497528076\n",
            "Epoch 8 Batch 84 loss 1.421985387802124\n",
            "Epoch 8 Batch 85 loss 1.437337040901184\n",
            "Epoch 8 Batch 86 loss 1.4805790185928345\n",
            "Epoch 8 Batch 87 loss 1.49105703830719\n",
            "Epoch 8 Batch 88 loss 1.4227960109710693\n",
            "Epoch 8 Batch 89 loss 1.4580776691436768\n",
            "Epoch 8 Batch 90 loss 1.4587750434875488\n",
            "Epoch 8 Batch 91 loss 1.4608750343322754\n",
            "Epoch 8 Batch 92 loss 1.4617657661437988\n",
            "Epoch 8 Loss 1.4627\n",
            "Time taken for 1 epoch 123.7561993598938 sec\n",
            "\n",
            "Epoch 9 Batch 0 loss 1.3542840480804443\n",
            "Epoch 9 Batch 1 loss 1.375486135482788\n",
            "Epoch 9 Batch 2 loss 1.3482071161270142\n",
            "Epoch 9 Batch 3 loss 1.37624192237854\n",
            "Epoch 9 Batch 4 loss 1.3696095943450928\n",
            "Epoch 9 Batch 5 loss 1.3360346555709839\n",
            "Epoch 9 Batch 6 loss 1.387144923210144\n",
            "Epoch 9 Batch 7 loss 1.3788939714431763\n",
            "Epoch 9 Batch 8 loss 1.3830455541610718\n",
            "Epoch 9 Batch 9 loss 1.380352258682251\n",
            "Epoch 9 Batch 10 loss 1.3589667081832886\n",
            "Epoch 9 Batch 11 loss 1.3979452848434448\n",
            "Epoch 9 Batch 12 loss 1.3898794651031494\n",
            "Epoch 9 Batch 13 loss 1.3449941873550415\n",
            "Epoch 9 Batch 14 loss 1.3479065895080566\n",
            "Epoch 9 Batch 15 loss 1.3468796014785767\n",
            "Epoch 9 Batch 16 loss 1.3640024662017822\n",
            "Epoch 9 Batch 17 loss 1.3463940620422363\n",
            "Epoch 9 Batch 18 loss 1.3690601587295532\n",
            "Epoch 9 Batch 19 loss 1.358258605003357\n",
            "Epoch 9 Batch 20 loss 1.3542120456695557\n",
            "Epoch 9 Batch 21 loss 1.3556926250457764\n",
            "Epoch 9 Batch 22 loss 1.3965671062469482\n",
            "Epoch 9 Batch 23 loss 1.3798635005950928\n",
            "Epoch 9 Batch 24 loss 1.3618931770324707\n",
            "Epoch 9 Batch 25 loss 1.3730766773223877\n",
            "Epoch 9 Batch 26 loss 1.3654170036315918\n",
            "Epoch 9 Batch 27 loss 1.3503632545471191\n",
            "Epoch 9 Batch 28 loss 1.4004114866256714\n",
            "Epoch 9 Batch 29 loss 1.3483465909957886\n",
            "Epoch 9 Batch 30 loss 1.3650009632110596\n",
            "Epoch 9 Batch 31 loss 1.389302372932434\n",
            "Epoch 9 Batch 32 loss 1.3380807638168335\n",
            "Epoch 9 Batch 33 loss 1.3792489767074585\n",
            "Epoch 9 Batch 34 loss 1.3533961772918701\n",
            "Epoch 9 Batch 35 loss 1.3530710935592651\n",
            "Epoch 9 Batch 36 loss 1.3752148151397705\n",
            "Epoch 9 Batch 37 loss 1.3582513332366943\n",
            "Epoch 9 Batch 38 loss 1.3598264455795288\n",
            "Epoch 9 Batch 39 loss 1.3568634986877441\n",
            "Epoch 9 Batch 40 loss 1.388221025466919\n",
            "Epoch 9 Batch 41 loss 1.4368306398391724\n",
            "Epoch 9 Batch 42 loss 1.3834773302078247\n",
            "Epoch 9 Batch 43 loss 1.337597370147705\n",
            "Epoch 9 Batch 44 loss 1.4042134284973145\n",
            "Epoch 9 Batch 45 loss 1.3299888372421265\n",
            "Epoch 9 Batch 46 loss 1.369888186454773\n",
            "Epoch 9 Batch 47 loss 1.3671294450759888\n",
            "Epoch 9 Batch 48 loss 1.366161823272705\n",
            "Epoch 9 Batch 49 loss 1.3837711811065674\n",
            "Epoch 9 Batch 50 loss 1.3828511238098145\n",
            "Epoch 9 Batch 51 loss 1.4006537199020386\n",
            "Epoch 9 Batch 52 loss 1.3334711790084839\n",
            "Epoch 9 Batch 53 loss 1.3746845722198486\n",
            "Epoch 9 Batch 54 loss 1.3513644933700562\n",
            "Epoch 9 Batch 55 loss 1.3476084470748901\n",
            "Epoch 9 Batch 56 loss 1.3374464511871338\n",
            "Epoch 9 Batch 57 loss 1.3485904932022095\n",
            "Epoch 9 Batch 58 loss 1.3740184307098389\n",
            "Epoch 9 Batch 59 loss 1.3501315116882324\n",
            "Epoch 9 Batch 60 loss 1.4016057252883911\n",
            "Epoch 9 Batch 61 loss 1.3417785167694092\n",
            "Epoch 9 Batch 62 loss 1.3414407968521118\n",
            "Epoch 9 Batch 63 loss 1.3746763467788696\n",
            "Epoch 9 Batch 64 loss 1.3909223079681396\n",
            "Epoch 9 Batch 65 loss 1.374670147895813\n",
            "Epoch 9 Batch 66 loss 1.337646722793579\n",
            "Epoch 9 Batch 67 loss 1.3898699283599854\n",
            "Epoch 9 Batch 68 loss 1.339894413948059\n",
            "Epoch 9 Batch 69 loss 1.3551685810089111\n",
            "Epoch 9 Batch 70 loss 1.3339029550552368\n",
            "Epoch 9 Batch 71 loss 1.3743693828582764\n",
            "Epoch 9 Batch 72 loss 1.3324949741363525\n",
            "Epoch 9 Batch 73 loss 1.352185606956482\n",
            "Epoch 9 Batch 74 loss 1.336784839630127\n",
            "Epoch 9 Batch 75 loss 1.3534691333770752\n",
            "Epoch 9 Batch 76 loss 1.3862240314483643\n",
            "Epoch 9 Batch 77 loss 1.370214581489563\n",
            "Epoch 9 Batch 78 loss 1.3736579418182373\n",
            "Epoch 9 Batch 79 loss 1.3247190713882446\n",
            "Epoch 9 Batch 80 loss 1.3903658390045166\n",
            "Epoch 9 Batch 81 loss 1.358749508857727\n",
            "Epoch 9 Batch 82 loss 1.3610271215438843\n",
            "Epoch 9 Batch 83 loss 1.3433735370635986\n",
            "Epoch 9 Batch 84 loss 1.3573006391525269\n",
            "Epoch 9 Batch 85 loss 1.3475247621536255\n",
            "Epoch 9 Batch 86 loss 1.3190016746520996\n",
            "Epoch 9 Batch 87 loss 1.3606632947921753\n",
            "Epoch 9 Batch 88 loss 1.3442034721374512\n",
            "Epoch 9 Batch 89 loss 1.3733365535736084\n",
            "Epoch 9 Batch 90 loss 1.3821309804916382\n",
            "Epoch 9 Batch 91 loss 1.365749478340149\n",
            "Epoch 9 Batch 92 loss 1.3659498691558838\n",
            "Epoch 9 Loss 1.3640\n",
            "Time taken for 1 epoch 122.11144828796387 sec\n",
            "\n",
            "Epoch 10 Batch 0 loss 1.2910878658294678\n",
            "Epoch 10 Batch 1 loss 1.2399518489837646\n",
            "Epoch 10 Batch 2 loss 1.2583411931991577\n",
            "Epoch 10 Batch 3 loss 1.2616103887557983\n",
            "Epoch 10 Batch 4 loss 1.2458394765853882\n",
            "Epoch 10 Batch 5 loss 1.2641429901123047\n",
            "Epoch 10 Batch 6 loss 1.2435147762298584\n",
            "Epoch 10 Batch 7 loss 1.2668501138687134\n",
            "Epoch 10 Batch 8 loss 1.26515531539917\n",
            "Epoch 10 Batch 9 loss 1.2464011907577515\n",
            "Epoch 10 Batch 10 loss 1.2469555139541626\n",
            "Epoch 10 Batch 11 loss 1.2955584526062012\n",
            "Epoch 10 Batch 12 loss 1.286589503288269\n",
            "Epoch 10 Batch 13 loss 1.268617033958435\n",
            "Epoch 10 Batch 14 loss 1.2308146953582764\n",
            "Epoch 10 Batch 15 loss 1.2402386665344238\n",
            "Epoch 10 Batch 16 loss 1.2706495523452759\n",
            "Epoch 10 Batch 17 loss 1.2556484937667847\n",
            "Epoch 10 Batch 18 loss 1.2470935583114624\n",
            "Epoch 10 Batch 19 loss 1.257184624671936\n",
            "Epoch 10 Batch 20 loss 1.267462134361267\n",
            "Epoch 10 Batch 21 loss 1.2665563821792603\n",
            "Epoch 10 Batch 22 loss 1.2704514265060425\n",
            "Epoch 10 Batch 23 loss 1.2854087352752686\n",
            "Epoch 10 Batch 24 loss 1.2806669473648071\n",
            "Epoch 10 Batch 25 loss 1.2738697528839111\n",
            "Epoch 10 Batch 26 loss 1.2685365676879883\n",
            "Epoch 10 Batch 27 loss 1.2657835483551025\n",
            "Epoch 10 Batch 28 loss 1.2659720182418823\n",
            "Epoch 10 Batch 29 loss 1.2536311149597168\n",
            "Epoch 10 Batch 30 loss 1.270580768585205\n",
            "Epoch 10 Batch 31 loss 1.2641499042510986\n",
            "Epoch 10 Batch 32 loss 1.2801320552825928\n",
            "Epoch 10 Batch 33 loss 1.26414155960083\n",
            "Epoch 10 Batch 34 loss 1.259383201599121\n",
            "Epoch 10 Batch 35 loss 1.293949842453003\n",
            "Epoch 10 Batch 36 loss 1.271166443824768\n",
            "Epoch 10 Batch 37 loss 1.2691541910171509\n",
            "Epoch 10 Batch 38 loss 1.2371442317962646\n",
            "Epoch 10 Batch 39 loss 1.2627089023590088\n",
            "Epoch 10 Batch 40 loss 1.2542837858200073\n",
            "Epoch 10 Batch 41 loss 1.3230489492416382\n",
            "Epoch 10 Batch 42 loss 1.2898998260498047\n",
            "Epoch 10 Batch 43 loss 1.301100254058838\n",
            "Epoch 10 Batch 44 loss 1.2787845134735107\n",
            "Epoch 10 Batch 45 loss 1.2898632287979126\n",
            "Epoch 10 Batch 46 loss 1.269476294517517\n",
            "Epoch 10 Batch 47 loss 1.267442226409912\n",
            "Epoch 10 Batch 48 loss 1.2607643604278564\n",
            "Epoch 10 Batch 49 loss 1.2585053443908691\n",
            "Epoch 10 Batch 50 loss 1.2597979307174683\n",
            "Epoch 10 Batch 51 loss 1.2682194709777832\n",
            "Epoch 10 Batch 52 loss 1.2984358072280884\n",
            "Epoch 10 Batch 53 loss 1.2481510639190674\n",
            "Epoch 10 Batch 54 loss 1.2766735553741455\n",
            "Epoch 10 Batch 55 loss 1.2604279518127441\n",
            "Epoch 10 Batch 56 loss 1.2824082374572754\n",
            "Epoch 10 Batch 57 loss 1.254900574684143\n",
            "Epoch 10 Batch 58 loss 1.2808818817138672\n",
            "Epoch 10 Batch 59 loss 1.2901182174682617\n",
            "Epoch 10 Batch 60 loss 1.253392219543457\n",
            "Epoch 10 Batch 61 loss 1.2626829147338867\n",
            "Epoch 10 Batch 62 loss 1.2925270795822144\n",
            "Epoch 10 Batch 63 loss 1.309669852256775\n",
            "Epoch 10 Batch 64 loss 1.2371193170547485\n",
            "Epoch 10 Batch 65 loss 1.250481128692627\n",
            "Epoch 10 Batch 66 loss 1.249450922012329\n",
            "Epoch 10 Batch 67 loss 1.2822513580322266\n",
            "Epoch 10 Batch 68 loss 1.25191330909729\n",
            "Epoch 10 Batch 69 loss 1.2434793710708618\n",
            "Epoch 10 Batch 70 loss 1.290643572807312\n",
            "Epoch 10 Batch 71 loss 1.2841196060180664\n",
            "Epoch 10 Batch 72 loss 1.2485346794128418\n",
            "Epoch 10 Batch 73 loss 1.3309123516082764\n",
            "Epoch 10 Batch 74 loss 1.2932449579238892\n",
            "Epoch 10 Batch 75 loss 1.2945988178253174\n",
            "Epoch 10 Batch 76 loss 1.257989764213562\n",
            "Epoch 10 Batch 77 loss 1.2517879009246826\n",
            "Epoch 10 Batch 78 loss 1.2907861471176147\n",
            "Epoch 10 Batch 79 loss 1.2415478229522705\n",
            "Epoch 10 Batch 80 loss 1.2602498531341553\n",
            "Epoch 10 Batch 81 loss 1.2916465997695923\n",
            "Epoch 10 Batch 82 loss 1.249523401260376\n",
            "Epoch 10 Batch 83 loss 1.287126064300537\n",
            "Epoch 10 Batch 84 loss 1.286177158355713\n",
            "Epoch 10 Batch 85 loss 1.2693277597427368\n",
            "Epoch 10 Batch 86 loss 1.251810908317566\n",
            "Epoch 10 Batch 87 loss 1.2480772733688354\n",
            "Epoch 10 Batch 88 loss 1.2876347303390503\n",
            "Epoch 10 Batch 89 loss 1.2625516653060913\n",
            "Epoch 10 Batch 90 loss 1.2609317302703857\n",
            "Epoch 10 Batch 91 loss 1.277203917503357\n",
            "Epoch 10 Batch 92 loss 1.2460458278656006\n",
            "Epoch 10 Loss 1.2684\n",
            "Time taken for 1 epoch 124.42171239852905 sec\n",
            "\n",
            "Epoch 11 Batch 0 loss 1.1753530502319336\n",
            "Epoch 11 Batch 1 loss 1.1503263711929321\n",
            "Epoch 11 Batch 2 loss 1.2077112197875977\n",
            "Epoch 11 Batch 3 loss 1.1737096309661865\n",
            "Epoch 11 Batch 4 loss 1.1653465032577515\n",
            "Epoch 11 Batch 5 loss 1.1667389869689941\n",
            "Epoch 11 Batch 6 loss 1.1775201559066772\n",
            "Epoch 11 Batch 7 loss 1.1607755422592163\n",
            "Epoch 11 Batch 8 loss 1.1430714130401611\n",
            "Epoch 11 Batch 9 loss 1.1812858581542969\n",
            "Epoch 11 Batch 10 loss 1.137771487236023\n",
            "Epoch 11 Batch 11 loss 1.1366119384765625\n",
            "Epoch 11 Batch 12 loss 1.162183165550232\n",
            "Epoch 11 Batch 13 loss 1.2056413888931274\n",
            "Epoch 11 Batch 14 loss 1.1782060861587524\n",
            "Epoch 11 Batch 15 loss 1.1585482358932495\n",
            "Epoch 11 Batch 16 loss 1.1786235570907593\n",
            "Epoch 11 Batch 17 loss 1.178398847579956\n",
            "Epoch 11 Batch 18 loss 1.129372477531433\n",
            "Epoch 11 Batch 19 loss 1.1588102579116821\n",
            "Epoch 11 Batch 20 loss 1.1645903587341309\n",
            "Epoch 11 Batch 21 loss 1.2371187210083008\n",
            "Epoch 11 Batch 22 loss 1.1287144422531128\n",
            "Epoch 11 Batch 23 loss 1.1581287384033203\n",
            "Epoch 11 Batch 24 loss 1.1791656017303467\n",
            "Epoch 11 Batch 25 loss 1.1852467060089111\n",
            "Epoch 11 Batch 26 loss 1.1938486099243164\n",
            "Epoch 11 Batch 27 loss 1.165899634361267\n",
            "Epoch 11 Batch 28 loss 1.1885218620300293\n",
            "Epoch 11 Batch 29 loss 1.152273416519165\n",
            "Epoch 11 Batch 30 loss 1.1874182224273682\n",
            "Epoch 11 Batch 31 loss 1.160054087638855\n",
            "Epoch 11 Batch 32 loss 1.1763534545898438\n",
            "Epoch 11 Batch 33 loss 1.1650644540786743\n",
            "Epoch 11 Batch 34 loss 1.1857619285583496\n",
            "Epoch 11 Batch 35 loss 1.1427206993103027\n",
            "Epoch 11 Batch 36 loss 1.1605523824691772\n",
            "Epoch 11 Batch 37 loss 1.1543644666671753\n",
            "Epoch 11 Batch 38 loss 1.1987406015396118\n",
            "Epoch 11 Batch 39 loss 1.1814006567001343\n",
            "Epoch 11 Batch 40 loss 1.1759179830551147\n",
            "Epoch 11 Batch 41 loss 1.1808098554611206\n",
            "Epoch 11 Batch 42 loss 1.1814839839935303\n",
            "Epoch 11 Batch 43 loss 1.1712045669555664\n",
            "Epoch 11 Batch 44 loss 1.2000316381454468\n",
            "Epoch 11 Batch 45 loss 1.199111819267273\n",
            "Epoch 11 Batch 46 loss 1.1875557899475098\n",
            "Epoch 11 Batch 47 loss 1.1961746215820312\n",
            "Epoch 11 Batch 48 loss 1.173515796661377\n",
            "Epoch 11 Batch 49 loss 1.1880968809127808\n",
            "Epoch 11 Batch 50 loss 1.1725271940231323\n",
            "Epoch 11 Batch 51 loss 1.1820720434188843\n",
            "Epoch 11 Batch 52 loss 1.18310546875\n",
            "Epoch 11 Batch 53 loss 1.166054129600525\n",
            "Epoch 11 Batch 54 loss 1.1836203336715698\n",
            "Epoch 11 Batch 55 loss 1.182950735092163\n",
            "Epoch 11 Batch 56 loss 1.1684468984603882\n",
            "Epoch 11 Batch 57 loss 1.2060989141464233\n",
            "Epoch 11 Batch 58 loss 1.1682898998260498\n",
            "Epoch 11 Batch 59 loss 1.1785812377929688\n",
            "Epoch 11 Batch 60 loss 1.1674580574035645\n",
            "Epoch 11 Batch 61 loss 1.189265251159668\n",
            "Epoch 11 Batch 62 loss 1.1752513647079468\n",
            "Epoch 11 Batch 63 loss 1.1656016111373901\n",
            "Epoch 11 Batch 64 loss 1.1773731708526611\n",
            "Epoch 11 Batch 65 loss 1.1700140237808228\n",
            "Epoch 11 Batch 66 loss 1.1764757633209229\n",
            "Epoch 11 Batch 67 loss 1.1747080087661743\n",
            "Epoch 11 Batch 68 loss 1.1482864618301392\n",
            "Epoch 11 Batch 69 loss 1.1632897853851318\n",
            "Epoch 11 Batch 70 loss 1.1739232540130615\n",
            "Epoch 11 Batch 71 loss 1.219340205192566\n",
            "Epoch 11 Batch 72 loss 1.1951799392700195\n",
            "Epoch 11 Batch 73 loss 1.1650599241256714\n",
            "Epoch 11 Batch 74 loss 1.1716461181640625\n",
            "Epoch 11 Batch 75 loss 1.156865119934082\n",
            "Epoch 11 Batch 76 loss 1.1792988777160645\n",
            "Epoch 11 Batch 77 loss 1.1754796504974365\n",
            "Epoch 11 Batch 78 loss 1.1899727582931519\n",
            "Epoch 11 Batch 79 loss 1.150153398513794\n",
            "Epoch 11 Batch 80 loss 1.1829018592834473\n",
            "Epoch 11 Batch 81 loss 1.1746900081634521\n",
            "Epoch 11 Batch 82 loss 1.1921703815460205\n",
            "Epoch 11 Batch 83 loss 1.1959415674209595\n",
            "Epoch 11 Batch 84 loss 1.1949089765548706\n",
            "Epoch 11 Batch 85 loss 1.1732690334320068\n",
            "Epoch 11 Batch 86 loss 1.151982069015503\n",
            "Epoch 11 Batch 87 loss 1.1591482162475586\n",
            "Epoch 11 Batch 88 loss 1.1526116132736206\n",
            "Epoch 11 Batch 89 loss 1.1764323711395264\n",
            "Epoch 11 Batch 90 loss 1.151816725730896\n",
            "Epoch 11 Batch 91 loss 1.2071528434753418\n",
            "Epoch 11 Batch 92 loss 1.190765142440796\n",
            "Epoch 11 Loss 1.1745\n",
            "Time taken for 1 epoch 122.10407829284668 sec\n",
            "\n",
            "Epoch 12 Batch 0 loss 1.0897611379623413\n",
            "Epoch 12 Batch 1 loss 1.0650943517684937\n",
            "Epoch 12 Batch 2 loss 1.076852560043335\n",
            "Epoch 12 Batch 3 loss 1.062303900718689\n",
            "Epoch 12 Batch 4 loss 1.0271042585372925\n",
            "Epoch 12 Batch 5 loss 1.0364127159118652\n",
            "Epoch 12 Batch 6 loss 1.0652227401733398\n",
            "Epoch 12 Batch 7 loss 1.0488026142120361\n",
            "Epoch 12 Batch 8 loss 1.0745279788970947\n",
            "Epoch 12 Batch 9 loss 1.036179780960083\n",
            "Epoch 12 Batch 10 loss 1.0856096744537354\n",
            "Epoch 12 Batch 11 loss 1.070642352104187\n",
            "Epoch 12 Batch 12 loss 1.057159662246704\n",
            "Epoch 12 Batch 13 loss 1.0604782104492188\n",
            "Epoch 12 Batch 14 loss 1.0476503372192383\n",
            "Epoch 12 Batch 15 loss 1.1012701988220215\n",
            "Epoch 12 Batch 16 loss 1.0717324018478394\n",
            "Epoch 12 Batch 17 loss 1.0496052503585815\n",
            "Epoch 12 Batch 18 loss 1.0642502307891846\n",
            "Epoch 12 Batch 19 loss 1.0977201461791992\n",
            "Epoch 12 Batch 20 loss 1.0607291460037231\n",
            "Epoch 12 Batch 21 loss 1.0614174604415894\n",
            "Epoch 12 Batch 22 loss 1.1041306257247925\n",
            "Epoch 12 Batch 23 loss 1.1132739782333374\n",
            "Epoch 12 Batch 24 loss 1.0595026016235352\n",
            "Epoch 12 Batch 25 loss 1.1223804950714111\n",
            "Epoch 12 Batch 26 loss 1.079547643661499\n",
            "Epoch 12 Batch 27 loss 1.0996265411376953\n",
            "Epoch 12 Batch 28 loss 1.0324759483337402\n",
            "Epoch 12 Batch 29 loss 1.063829779624939\n",
            "Epoch 12 Batch 30 loss 1.0813101530075073\n",
            "Epoch 12 Batch 31 loss 1.0899176597595215\n",
            "Epoch 12 Batch 32 loss 1.089665174484253\n",
            "Epoch 12 Batch 33 loss 1.1011863946914673\n",
            "Epoch 12 Batch 34 loss 1.0603100061416626\n",
            "Epoch 12 Batch 35 loss 1.0802364349365234\n",
            "Epoch 12 Batch 36 loss 1.0814130306243896\n",
            "Epoch 12 Batch 37 loss 1.061224102973938\n",
            "Epoch 12 Batch 38 loss 1.0881202220916748\n",
            "Epoch 12 Batch 39 loss 1.0853753089904785\n",
            "Epoch 12 Batch 40 loss 1.1180709600448608\n",
            "Epoch 12 Batch 41 loss 1.048946738243103\n",
            "Epoch 12 Batch 42 loss 1.0914177894592285\n",
            "Epoch 12 Batch 43 loss 1.1092246770858765\n",
            "Epoch 12 Batch 44 loss 1.1181074380874634\n",
            "Epoch 12 Batch 45 loss 1.0892280340194702\n",
            "Epoch 12 Batch 46 loss 1.0971624851226807\n",
            "Epoch 12 Batch 47 loss 1.0590914487838745\n",
            "Epoch 12 Batch 48 loss 1.0739234685897827\n",
            "Epoch 12 Batch 49 loss 1.091691255569458\n",
            "Epoch 12 Batch 50 loss 1.1085448265075684\n",
            "Epoch 12 Batch 51 loss 1.094010829925537\n",
            "Epoch 12 Batch 52 loss 1.1002600193023682\n",
            "Epoch 12 Batch 53 loss 1.0718592405319214\n",
            "Epoch 12 Batch 54 loss 1.1021305322647095\n",
            "Epoch 12 Batch 55 loss 1.10077702999115\n",
            "Epoch 12 Batch 56 loss 1.097755789756775\n",
            "Epoch 12 Batch 57 loss 1.083997368812561\n",
            "Epoch 12 Batch 58 loss 1.0836985111236572\n",
            "Epoch 12 Batch 59 loss 1.0862079858779907\n",
            "Epoch 12 Batch 60 loss 1.0802971124649048\n",
            "Epoch 12 Batch 61 loss 1.1162022352218628\n",
            "Epoch 12 Batch 62 loss 1.1142305135726929\n",
            "Epoch 12 Batch 63 loss 1.073012113571167\n",
            "Epoch 12 Batch 64 loss 1.0801260471343994\n",
            "Epoch 12 Batch 65 loss 1.0705788135528564\n",
            "Epoch 12 Batch 66 loss 1.0595412254333496\n",
            "Epoch 12 Batch 67 loss 1.0976264476776123\n",
            "Epoch 12 Batch 68 loss 1.0686841011047363\n",
            "Epoch 12 Batch 69 loss 1.0749485492706299\n",
            "Epoch 12 Batch 70 loss 1.093395709991455\n",
            "Epoch 12 Batch 71 loss 1.0926140546798706\n",
            "Epoch 12 Batch 72 loss 1.1060938835144043\n",
            "Epoch 12 Batch 73 loss 1.0814787149429321\n",
            "Epoch 12 Batch 74 loss 1.094581127166748\n",
            "Epoch 12 Batch 75 loss 1.0613031387329102\n",
            "Epoch 12 Batch 76 loss 1.098456859588623\n",
            "Epoch 12 Batch 77 loss 1.109821081161499\n",
            "Epoch 12 Batch 78 loss 1.0917611122131348\n",
            "Epoch 12 Batch 79 loss 1.0719749927520752\n",
            "Epoch 12 Batch 80 loss 1.0968165397644043\n",
            "Epoch 12 Batch 81 loss 1.0941816568374634\n",
            "Epoch 12 Batch 82 loss 1.0927648544311523\n",
            "Epoch 12 Batch 83 loss 1.0931789875030518\n",
            "Epoch 12 Batch 84 loss 1.0738426446914673\n",
            "Epoch 12 Batch 85 loss 1.079917311668396\n",
            "Epoch 12 Batch 86 loss 1.0992332696914673\n",
            "Epoch 12 Batch 87 loss 1.1117451190948486\n",
            "Epoch 12 Batch 88 loss 1.0819143056869507\n",
            "Epoch 12 Batch 89 loss 1.1013875007629395\n",
            "Epoch 12 Batch 90 loss 1.0727999210357666\n",
            "Epoch 12 Batch 91 loss 1.0799065828323364\n",
            "Epoch 12 Batch 92 loss 1.0861201286315918\n",
            "Epoch 12 Loss 1.0821\n",
            "Time taken for 1 epoch 124.17620468139648 sec\n",
            "\n",
            "Epoch 13 Batch 0 loss 0.9558866024017334\n",
            "Epoch 13 Batch 1 loss 0.9771839380264282\n",
            "Epoch 13 Batch 2 loss 0.9518731832504272\n",
            "Epoch 13 Batch 3 loss 0.993889331817627\n",
            "Epoch 13 Batch 4 loss 0.9683252573013306\n",
            "Epoch 13 Batch 5 loss 0.9832127094268799\n",
            "Epoch 13 Batch 6 loss 0.9425811767578125\n",
            "Epoch 13 Batch 7 loss 0.951860785484314\n",
            "Epoch 13 Batch 8 loss 0.9478097558021545\n",
            "Epoch 13 Batch 9 loss 0.9937402606010437\n",
            "Epoch 13 Batch 10 loss 0.9829925298690796\n",
            "Epoch 13 Batch 11 loss 0.9978324770927429\n",
            "Epoch 13 Batch 12 loss 0.9701038002967834\n",
            "Epoch 13 Batch 13 loss 0.9982249736785889\n",
            "Epoch 13 Batch 14 loss 0.9552944898605347\n",
            "Epoch 13 Batch 15 loss 1.013835072517395\n",
            "Epoch 13 Batch 16 loss 0.9961111545562744\n",
            "Epoch 13 Batch 17 loss 0.9790263772010803\n",
            "Epoch 13 Batch 18 loss 0.9751263856887817\n",
            "Epoch 13 Batch 19 loss 1.0106903314590454\n",
            "Epoch 13 Batch 20 loss 0.965453028678894\n",
            "Epoch 13 Batch 21 loss 0.9644752740859985\n",
            "Epoch 13 Batch 22 loss 0.9749823808670044\n",
            "Epoch 13 Batch 23 loss 0.9680743217468262\n",
            "Epoch 13 Batch 24 loss 1.0182857513427734\n",
            "Epoch 13 Batch 25 loss 0.9908204674720764\n",
            "Epoch 13 Batch 26 loss 0.9945326447486877\n",
            "Epoch 13 Batch 27 loss 0.9820405840873718\n",
            "Epoch 13 Batch 28 loss 0.9927105903625488\n",
            "Epoch 13 Batch 29 loss 1.0116287469863892\n",
            "Epoch 13 Batch 30 loss 0.9811835885047913\n",
            "Epoch 13 Batch 31 loss 1.0282386541366577\n",
            "Epoch 13 Batch 32 loss 0.9772027134895325\n",
            "Epoch 13 Batch 33 loss 1.0042705535888672\n",
            "Epoch 13 Batch 34 loss 1.0115818977355957\n",
            "Epoch 13 Batch 35 loss 1.0293670892715454\n",
            "Epoch 13 Batch 36 loss 1.0259979963302612\n",
            "Epoch 13 Batch 37 loss 0.9976210594177246\n",
            "Epoch 13 Batch 38 loss 0.9859527349472046\n",
            "Epoch 13 Batch 39 loss 0.9996050000190735\n",
            "Epoch 13 Batch 40 loss 0.9987691044807434\n",
            "Epoch 13 Batch 41 loss 1.0265635251998901\n",
            "Epoch 13 Batch 42 loss 1.0103672742843628\n",
            "Epoch 13 Batch 43 loss 0.9991689324378967\n",
            "Epoch 13 Batch 44 loss 1.023055911064148\n",
            "Epoch 13 Batch 45 loss 0.9978190660476685\n",
            "Epoch 13 Batch 46 loss 0.990328311920166\n",
            "Epoch 13 Batch 47 loss 0.9774665832519531\n",
            "Epoch 13 Batch 48 loss 0.9781368970870972\n",
            "Epoch 13 Batch 49 loss 1.012115716934204\n",
            "Epoch 13 Batch 50 loss 0.9951613545417786\n",
            "Epoch 13 Batch 51 loss 0.9949484467506409\n",
            "Epoch 13 Batch 52 loss 1.0352494716644287\n",
            "Epoch 13 Batch 53 loss 1.0328633785247803\n",
            "Epoch 13 Batch 54 loss 1.0047091245651245\n",
            "Epoch 13 Batch 55 loss 1.0119154453277588\n",
            "Epoch 13 Batch 56 loss 0.986542820930481\n",
            "Epoch 13 Batch 57 loss 0.9978101849555969\n",
            "Epoch 13 Batch 58 loss 0.9908145666122437\n",
            "Epoch 13 Batch 59 loss 1.0171117782592773\n",
            "Epoch 13 Batch 60 loss 1.0082218647003174\n",
            "Epoch 13 Batch 61 loss 1.002755045890808\n",
            "Epoch 13 Batch 62 loss 1.008554458618164\n",
            "Epoch 13 Batch 63 loss 1.0013397932052612\n",
            "Epoch 13 Batch 64 loss 1.0107849836349487\n",
            "Epoch 13 Batch 65 loss 1.0011132955551147\n",
            "Epoch 13 Batch 66 loss 0.986717939376831\n",
            "Epoch 13 Batch 67 loss 1.007059931755066\n",
            "Epoch 13 Batch 68 loss 0.977519690990448\n",
            "Epoch 13 Batch 69 loss 0.9725485444068909\n",
            "Epoch 13 Batch 70 loss 0.9835052490234375\n",
            "Epoch 13 Batch 71 loss 1.0180211067199707\n",
            "Epoch 13 Batch 72 loss 0.9986339211463928\n",
            "Epoch 13 Batch 73 loss 1.0040998458862305\n",
            "Epoch 13 Batch 74 loss 1.0362098217010498\n",
            "Epoch 13 Batch 75 loss 1.017653465270996\n",
            "Epoch 13 Batch 76 loss 1.006690263748169\n",
            "Epoch 13 Batch 77 loss 1.0060502290725708\n",
            "Epoch 13 Batch 78 loss 1.00654935836792\n",
            "Epoch 13 Batch 79 loss 1.0061452388763428\n",
            "Epoch 13 Batch 80 loss 1.001765251159668\n",
            "Epoch 13 Batch 81 loss 1.0022549629211426\n",
            "Epoch 13 Batch 82 loss 1.0023397207260132\n",
            "Epoch 13 Batch 83 loss 1.0255810022354126\n",
            "Epoch 13 Batch 84 loss 0.994098424911499\n",
            "Epoch 13 Batch 85 loss 1.0258874893188477\n",
            "Epoch 13 Batch 86 loss 1.0291234254837036\n",
            "Epoch 13 Batch 87 loss 1.014147162437439\n",
            "Epoch 13 Batch 88 loss 0.9916707277297974\n",
            "Epoch 13 Batch 89 loss 0.9873367547988892\n",
            "Epoch 13 Batch 90 loss 0.9890819787979126\n",
            "Epoch 13 Batch 91 loss 0.9984648823738098\n",
            "Epoch 13 Batch 92 loss 1.0132521390914917\n",
            "Epoch 13 Loss 0.9964\n",
            "Time taken for 1 epoch 122.35337209701538 sec\n",
            "\n",
            "Epoch 14 Batch 0 loss 0.8931573629379272\n",
            "Epoch 14 Batch 1 loss 0.908473014831543\n",
            "Epoch 14 Batch 2 loss 0.9079158902168274\n",
            "Epoch 14 Batch 3 loss 0.8806291222572327\n",
            "Epoch 14 Batch 4 loss 0.8865137100219727\n",
            "Epoch 14 Batch 5 loss 0.9044948816299438\n",
            "Epoch 14 Batch 6 loss 0.8776665925979614\n",
            "Epoch 14 Batch 7 loss 0.8917668461799622\n",
            "Epoch 14 Batch 8 loss 0.920733630657196\n",
            "Epoch 14 Batch 9 loss 0.8870633244514465\n",
            "Epoch 14 Batch 10 loss 0.8937180638313293\n",
            "Epoch 14 Batch 11 loss 0.9305137395858765\n",
            "Epoch 14 Batch 12 loss 0.8927243947982788\n",
            "Epoch 14 Batch 13 loss 0.8977385759353638\n",
            "Epoch 14 Batch 14 loss 0.9099347591400146\n",
            "Epoch 14 Batch 15 loss 0.8998839259147644\n",
            "Epoch 14 Batch 16 loss 0.8967777490615845\n",
            "Epoch 14 Batch 17 loss 0.8879302144050598\n",
            "Epoch 14 Batch 18 loss 0.8854060173034668\n",
            "Epoch 14 Batch 19 loss 0.9009190201759338\n",
            "Epoch 14 Batch 20 loss 0.8884382247924805\n",
            "Epoch 14 Batch 21 loss 0.8873313665390015\n",
            "Epoch 14 Batch 22 loss 0.9319003820419312\n",
            "Epoch 14 Batch 23 loss 0.8875141143798828\n",
            "Epoch 14 Batch 24 loss 0.8997055888175964\n",
            "Epoch 14 Batch 25 loss 0.8966878056526184\n",
            "Epoch 14 Batch 26 loss 0.8694065809249878\n",
            "Epoch 14 Batch 27 loss 0.9167730808258057\n",
            "Epoch 14 Batch 28 loss 0.9023764729499817\n",
            "Epoch 14 Batch 29 loss 0.8909395933151245\n",
            "Epoch 14 Batch 30 loss 0.9095777273178101\n",
            "Epoch 14 Batch 31 loss 0.889519214630127\n",
            "Epoch 14 Batch 32 loss 0.927556037902832\n",
            "Epoch 14 Batch 33 loss 0.9281390309333801\n",
            "Epoch 14 Batch 34 loss 0.9187820553779602\n",
            "Epoch 14 Batch 35 loss 0.9342937469482422\n",
            "Epoch 14 Batch 36 loss 0.9219980835914612\n",
            "Epoch 14 Batch 37 loss 0.916159451007843\n",
            "Epoch 14 Batch 38 loss 0.8972227573394775\n",
            "Epoch 14 Batch 39 loss 0.9425630569458008\n",
            "Epoch 14 Batch 40 loss 0.9534354209899902\n",
            "Epoch 14 Batch 41 loss 0.9048992395401001\n",
            "Epoch 14 Batch 42 loss 0.919477641582489\n",
            "Epoch 14 Batch 43 loss 0.9104669690132141\n",
            "Epoch 14 Batch 44 loss 0.9011512994766235\n",
            "Epoch 14 Batch 45 loss 0.9372421503067017\n",
            "Epoch 14 Batch 46 loss 0.9266475439071655\n",
            "Epoch 14 Batch 47 loss 0.926074743270874\n",
            "Epoch 14 Batch 48 loss 0.9274718165397644\n",
            "Epoch 14 Batch 49 loss 0.9192573428153992\n",
            "Epoch 14 Batch 50 loss 0.9654356241226196\n",
            "Epoch 14 Batch 51 loss 0.9280574321746826\n",
            "Epoch 14 Batch 52 loss 0.9085952639579773\n",
            "Epoch 14 Batch 53 loss 0.9324148297309875\n",
            "Epoch 14 Batch 54 loss 0.9354726672172546\n",
            "Epoch 14 Batch 55 loss 0.922441303730011\n",
            "Epoch 14 Batch 56 loss 0.9167838096618652\n",
            "Epoch 14 Batch 57 loss 0.8974103927612305\n",
            "Epoch 14 Batch 58 loss 0.9294067025184631\n",
            "Epoch 14 Batch 59 loss 0.9216042160987854\n",
            "Epoch 14 Batch 60 loss 0.9057871699333191\n",
            "Epoch 14 Batch 61 loss 0.9194450378417969\n",
            "Epoch 14 Batch 62 loss 0.9290885925292969\n",
            "Epoch 14 Batch 63 loss 0.9090946316719055\n",
            "Epoch 14 Batch 64 loss 0.9088811874389648\n",
            "Epoch 14 Batch 65 loss 0.9281933903694153\n",
            "Epoch 14 Batch 66 loss 0.9169798493385315\n",
            "Epoch 14 Batch 67 loss 0.9268511533737183\n",
            "Epoch 14 Batch 68 loss 0.9456585049629211\n",
            "Epoch 14 Batch 69 loss 0.9301634430885315\n",
            "Epoch 14 Batch 70 loss 0.9277456998825073\n",
            "Epoch 14 Batch 71 loss 0.9077650904655457\n",
            "Epoch 14 Batch 72 loss 0.9251352548599243\n",
            "Epoch 14 Batch 73 loss 0.9227179288864136\n",
            "Epoch 14 Batch 74 loss 0.9060360193252563\n",
            "Epoch 14 Batch 75 loss 0.9357779026031494\n",
            "Epoch 14 Batch 76 loss 0.9095586538314819\n",
            "Epoch 14 Batch 77 loss 0.925147294998169\n",
            "Epoch 14 Batch 78 loss 0.9038105010986328\n",
            "Epoch 14 Batch 79 loss 0.9188382625579834\n",
            "Epoch 14 Batch 80 loss 0.9002652168273926\n",
            "Epoch 14 Batch 81 loss 0.9413730502128601\n",
            "Epoch 14 Batch 82 loss 0.9151249527931213\n",
            "Epoch 14 Batch 83 loss 0.9343793392181396\n",
            "Epoch 14 Batch 84 loss 0.9417179226875305\n",
            "Epoch 14 Batch 85 loss 0.9516335725784302\n",
            "Epoch 14 Batch 86 loss 0.9234865307807922\n",
            "Epoch 14 Batch 87 loss 0.9141045808792114\n",
            "Epoch 14 Batch 88 loss 0.9162169694900513\n",
            "Epoch 14 Batch 89 loss 0.9304143190383911\n",
            "Epoch 14 Batch 90 loss 0.9436469078063965\n",
            "Epoch 14 Batch 91 loss 0.9430687427520752\n",
            "Epoch 14 Batch 92 loss 0.9023547172546387\n",
            "Epoch 14 Loss 0.9146\n",
            "Time taken for 1 epoch 123.77604007720947 sec\n",
            "\n",
            "Epoch 15 Batch 0 loss 0.8257060647010803\n",
            "Epoch 15 Batch 1 loss 0.830909013748169\n",
            "Epoch 15 Batch 2 loss 0.8004595041275024\n",
            "Epoch 15 Batch 3 loss 0.8139464855194092\n",
            "Epoch 15 Batch 4 loss 0.8036748170852661\n",
            "Epoch 15 Batch 5 loss 0.8366209864616394\n",
            "Epoch 15 Batch 6 loss 0.8031598329544067\n",
            "Epoch 15 Batch 7 loss 0.8244629502296448\n",
            "Epoch 15 Batch 8 loss 0.8276636600494385\n",
            "Epoch 15 Batch 9 loss 0.8158776164054871\n",
            "Epoch 15 Batch 10 loss 0.8315756916999817\n",
            "Epoch 15 Batch 11 loss 0.8189400434494019\n",
            "Epoch 15 Batch 12 loss 0.8386775255203247\n",
            "Epoch 15 Batch 13 loss 0.8159623742103577\n",
            "Epoch 15 Batch 14 loss 0.803911566734314\n",
            "Epoch 15 Batch 15 loss 0.827335000038147\n",
            "Epoch 15 Batch 16 loss 0.8072472810745239\n",
            "Epoch 15 Batch 17 loss 0.8200021386146545\n",
            "Epoch 15 Batch 18 loss 0.8270719647407532\n",
            "Epoch 15 Batch 19 loss 0.8262792825698853\n",
            "Epoch 15 Batch 20 loss 0.8374360203742981\n",
            "Epoch 15 Batch 21 loss 0.8125929236412048\n",
            "Epoch 15 Batch 22 loss 0.8294013142585754\n",
            "Epoch 15 Batch 23 loss 0.8330258131027222\n",
            "Epoch 15 Batch 24 loss 0.8405075669288635\n",
            "Epoch 15 Batch 25 loss 0.8394168615341187\n",
            "Epoch 15 Batch 26 loss 0.8194540739059448\n",
            "Epoch 15 Batch 27 loss 0.8075629472732544\n",
            "Epoch 15 Batch 28 loss 0.8282120227813721\n",
            "Epoch 15 Batch 29 loss 0.832969605922699\n",
            "Epoch 15 Batch 30 loss 0.8343842029571533\n",
            "Epoch 15 Batch 31 loss 0.8186066150665283\n",
            "Epoch 15 Batch 32 loss 0.8491008877754211\n",
            "Epoch 15 Batch 33 loss 0.8216423392295837\n",
            "Epoch 15 Batch 34 loss 0.827445924282074\n",
            "Epoch 15 Batch 35 loss 0.7997936606407166\n",
            "Epoch 15 Batch 36 loss 0.8326758146286011\n",
            "Epoch 15 Batch 37 loss 0.8174368739128113\n",
            "Epoch 15 Batch 38 loss 0.8631727695465088\n",
            "Epoch 15 Batch 39 loss 0.8223909139633179\n",
            "Epoch 15 Batch 40 loss 0.8644176721572876\n",
            "Epoch 15 Batch 41 loss 0.8356162309646606\n",
            "Epoch 15 Batch 42 loss 0.8482498526573181\n",
            "Epoch 15 Batch 43 loss 0.825840175151825\n",
            "Epoch 15 Batch 44 loss 0.8191203474998474\n",
            "Epoch 15 Batch 45 loss 0.8415893316268921\n",
            "Epoch 15 Batch 46 loss 0.8072933554649353\n",
            "Epoch 15 Batch 47 loss 0.8594613075256348\n",
            "Epoch 15 Batch 48 loss 0.8380600810050964\n",
            "Epoch 15 Batch 49 loss 0.8450218439102173\n",
            "Epoch 15 Batch 50 loss 0.8426584005355835\n",
            "Epoch 15 Batch 51 loss 0.8369891047477722\n",
            "Epoch 15 Batch 52 loss 0.8426544070243835\n",
            "Epoch 15 Batch 53 loss 0.8181130886077881\n",
            "Epoch 15 Batch 54 loss 0.8582716584205627\n",
            "Epoch 15 Batch 55 loss 0.8748337626457214\n",
            "Epoch 15 Batch 56 loss 0.8319519758224487\n",
            "Epoch 15 Batch 57 loss 0.8201732039451599\n",
            "Epoch 15 Batch 58 loss 0.8541269898414612\n",
            "Epoch 15 Batch 59 loss 0.867190420627594\n",
            "Epoch 15 Batch 60 loss 0.843323826789856\n",
            "Epoch 15 Batch 61 loss 0.8481062054634094\n",
            "Epoch 15 Batch 62 loss 0.8378716111183167\n",
            "Epoch 15 Batch 63 loss 0.8787654042243958\n",
            "Epoch 15 Batch 64 loss 0.8412652611732483\n",
            "Epoch 15 Batch 65 loss 0.8325463533401489\n",
            "Epoch 15 Batch 66 loss 0.8151494264602661\n",
            "Epoch 15 Batch 67 loss 0.8455380201339722\n",
            "Epoch 15 Batch 68 loss 0.8738986253738403\n",
            "Epoch 15 Batch 69 loss 0.8620215654373169\n",
            "Epoch 15 Batch 70 loss 0.8659722208976746\n",
            "Epoch 15 Batch 71 loss 0.8834089040756226\n",
            "Epoch 15 Batch 72 loss 0.8418994545936584\n",
            "Epoch 15 Batch 73 loss 0.8393840193748474\n",
            "Epoch 15 Batch 74 loss 0.8680804967880249\n",
            "Epoch 15 Batch 75 loss 0.8357096910476685\n",
            "Epoch 15 Batch 76 loss 0.8296358585357666\n",
            "Epoch 15 Batch 77 loss 0.8540419340133667\n",
            "Epoch 15 Batch 78 loss 0.8397024273872375\n",
            "Epoch 15 Batch 79 loss 0.8555576205253601\n",
            "Epoch 15 Batch 80 loss 0.8453642725944519\n",
            "Epoch 15 Batch 81 loss 0.8554803133010864\n",
            "Epoch 15 Batch 82 loss 0.8620729446411133\n",
            "Epoch 15 Batch 83 loss 0.8858076930046082\n",
            "Epoch 15 Batch 84 loss 0.8232003450393677\n",
            "Epoch 15 Batch 85 loss 0.8191041350364685\n",
            "Epoch 15 Batch 86 loss 0.8652698993682861\n",
            "Epoch 15 Batch 87 loss 0.82654869556427\n",
            "Epoch 15 Batch 88 loss 0.8564268350601196\n",
            "Epoch 15 Batch 89 loss 0.8327676057815552\n",
            "Epoch 15 Batch 90 loss 0.8516604900360107\n",
            "Epoch 15 Batch 91 loss 0.8366249799728394\n",
            "Epoch 15 Batch 92 loss 0.8155331015586853\n",
            "Epoch 15 Loss 0.8362\n",
            "Time taken for 1 epoch 122.03734588623047 sec\n",
            "\n",
            "Epoch 16 Batch 0 loss 0.7527870535850525\n",
            "Epoch 16 Batch 1 loss 0.7187952399253845\n",
            "Epoch 16 Batch 2 loss 0.7326399683952332\n",
            "Epoch 16 Batch 3 loss 0.7219077944755554\n",
            "Epoch 16 Batch 4 loss 0.7425939440727234\n",
            "Epoch 16 Batch 5 loss 0.7336681485176086\n",
            "Epoch 16 Batch 6 loss 0.7066857218742371\n",
            "Epoch 16 Batch 7 loss 0.7590038180351257\n",
            "Epoch 16 Batch 8 loss 0.7344947457313538\n",
            "Epoch 16 Batch 9 loss 0.7234222888946533\n",
            "Epoch 16 Batch 10 loss 0.695010244846344\n",
            "Epoch 16 Batch 11 loss 0.737711489200592\n",
            "Epoch 16 Batch 12 loss 0.7460809946060181\n",
            "Epoch 16 Batch 13 loss 0.7526276707649231\n",
            "Epoch 16 Batch 14 loss 0.7430722117424011\n",
            "Epoch 16 Batch 15 loss 0.7583926916122437\n",
            "Epoch 16 Batch 16 loss 0.7421119809150696\n",
            "Epoch 16 Batch 17 loss 0.7456886172294617\n",
            "Epoch 16 Batch 18 loss 0.7522679567337036\n",
            "Epoch 16 Batch 19 loss 0.7399076819419861\n",
            "Epoch 16 Batch 20 loss 0.7511466145515442\n",
            "Epoch 16 Batch 21 loss 0.7584161162376404\n",
            "Epoch 16 Batch 22 loss 0.7449800372123718\n",
            "Epoch 16 Batch 23 loss 0.7607445120811462\n",
            "Epoch 16 Batch 24 loss 0.7531566023826599\n",
            "Epoch 16 Batch 25 loss 0.7374629378318787\n",
            "Epoch 16 Batch 26 loss 0.7678118348121643\n",
            "Epoch 16 Batch 27 loss 0.783107578754425\n",
            "Epoch 16 Batch 28 loss 0.7584644556045532\n",
            "Epoch 16 Batch 29 loss 0.7463328838348389\n",
            "Epoch 16 Batch 30 loss 0.7366244792938232\n",
            "Epoch 16 Batch 31 loss 0.7537497282028198\n",
            "Epoch 16 Batch 32 loss 0.759537398815155\n",
            "Epoch 16 Batch 33 loss 0.7197285890579224\n",
            "Epoch 16 Batch 34 loss 0.7750512957572937\n",
            "Epoch 16 Batch 35 loss 0.7735925316810608\n",
            "Epoch 16 Batch 36 loss 0.7712122797966003\n",
            "Epoch 16 Batch 37 loss 0.7732397317886353\n",
            "Epoch 16 Batch 38 loss 0.7209328413009644\n",
            "Epoch 16 Batch 39 loss 0.7618958950042725\n",
            "Epoch 16 Batch 40 loss 0.7454662919044495\n",
            "Epoch 16 Batch 41 loss 0.7618439197540283\n",
            "Epoch 16 Batch 42 loss 0.7400075197219849\n",
            "Epoch 16 Batch 43 loss 0.7581440806388855\n",
            "Epoch 16 Batch 44 loss 0.7718592882156372\n",
            "Epoch 16 Batch 45 loss 0.7582489252090454\n",
            "Epoch 16 Batch 46 loss 0.7818798422813416\n",
            "Epoch 16 Batch 47 loss 0.7842724919319153\n",
            "Epoch 16 Batch 48 loss 0.7678952813148499\n",
            "Epoch 16 Batch 49 loss 0.7525623440742493\n",
            "Epoch 16 Batch 50 loss 0.7873817086219788\n",
            "Epoch 16 Batch 51 loss 0.7570759654045105\n",
            "Epoch 16 Batch 52 loss 0.7391799092292786\n",
            "Epoch 16 Batch 53 loss 0.7943353056907654\n",
            "Epoch 16 Batch 54 loss 0.7920374274253845\n",
            "Epoch 16 Batch 55 loss 0.7893063426017761\n",
            "Epoch 16 Batch 56 loss 0.7616699934005737\n",
            "Epoch 16 Batch 57 loss 0.7863484621047974\n",
            "Epoch 16 Batch 58 loss 0.7617978453636169\n",
            "Epoch 16 Batch 59 loss 0.7618039846420288\n",
            "Epoch 16 Batch 60 loss 0.765065610408783\n",
            "Epoch 16 Batch 61 loss 0.7722041606903076\n",
            "Epoch 16 Batch 62 loss 0.7898210883140564\n",
            "Epoch 16 Batch 63 loss 0.7740819454193115\n",
            "Epoch 16 Batch 64 loss 0.7776690125465393\n",
            "Epoch 16 Batch 65 loss 0.733239471912384\n",
            "Epoch 16 Batch 66 loss 0.7558225393295288\n",
            "Epoch 16 Batch 67 loss 0.777859628200531\n",
            "Epoch 16 Batch 68 loss 0.7848973870277405\n",
            "Epoch 16 Batch 69 loss 0.7888352870941162\n",
            "Epoch 16 Batch 70 loss 0.7712304592132568\n",
            "Epoch 16 Batch 71 loss 0.7518109083175659\n",
            "Epoch 16 Batch 72 loss 0.8034250736236572\n",
            "Epoch 16 Batch 73 loss 0.7840260863304138\n",
            "Epoch 16 Batch 74 loss 0.7701451778411865\n",
            "Epoch 16 Batch 75 loss 0.7750207185745239\n",
            "Epoch 16 Batch 76 loss 0.7949233651161194\n",
            "Epoch 16 Batch 77 loss 0.7791314721107483\n",
            "Epoch 16 Batch 78 loss 0.77311110496521\n",
            "Epoch 16 Batch 79 loss 0.7945473790168762\n",
            "Epoch 16 Batch 80 loss 0.7672932147979736\n",
            "Epoch 16 Batch 81 loss 0.7980653643608093\n",
            "Epoch 16 Batch 82 loss 0.7625590562820435\n",
            "Epoch 16 Batch 83 loss 0.7987363934516907\n",
            "Epoch 16 Batch 84 loss 0.7815371751785278\n",
            "Epoch 16 Batch 85 loss 0.776493489742279\n",
            "Epoch 16 Batch 86 loss 0.7813596129417419\n",
            "Epoch 16 Batch 87 loss 0.8028136491775513\n",
            "Epoch 16 Batch 88 loss 0.7871472239494324\n",
            "Epoch 16 Batch 89 loss 0.7616003751754761\n",
            "Epoch 16 Batch 90 loss 0.78548663854599\n",
            "Epoch 16 Batch 91 loss 0.7698878049850464\n",
            "Epoch 16 Batch 92 loss 0.7919291853904724\n",
            "Epoch 16 Loss 0.7622\n",
            "Time taken for 1 epoch 124.33403992652893 sec\n",
            "\n",
            "Epoch 17 Batch 0 loss 0.6859414577484131\n",
            "Epoch 17 Batch 1 loss 0.6477192640304565\n",
            "Epoch 17 Batch 2 loss 0.666835606098175\n",
            "Epoch 17 Batch 3 loss 0.6508244276046753\n",
            "Epoch 17 Batch 4 loss 0.6592056751251221\n",
            "Epoch 17 Batch 5 loss 0.6518316268920898\n",
            "Epoch 17 Batch 6 loss 0.6395630240440369\n",
            "Epoch 17 Batch 7 loss 0.6849062442779541\n",
            "Epoch 17 Batch 8 loss 0.6559596061706543\n",
            "Epoch 17 Batch 9 loss 0.6804563403129578\n",
            "Epoch 17 Batch 10 loss 0.6649186611175537\n",
            "Epoch 17 Batch 11 loss 0.6779172420501709\n",
            "Epoch 17 Batch 12 loss 0.6716195940971375\n",
            "Epoch 17 Batch 13 loss 0.6945782899856567\n",
            "Epoch 17 Batch 14 loss 0.6694352626800537\n",
            "Epoch 17 Batch 15 loss 0.6692016124725342\n",
            "Epoch 17 Batch 16 loss 0.6638580560684204\n",
            "Epoch 17 Batch 17 loss 0.6812053918838501\n",
            "Epoch 17 Batch 18 loss 0.6748391389846802\n",
            "Epoch 17 Batch 19 loss 0.65911465883255\n",
            "Epoch 17 Batch 20 loss 0.6938797831535339\n",
            "Epoch 17 Batch 21 loss 0.6918167471885681\n",
            "Epoch 17 Batch 22 loss 0.664162814617157\n",
            "Epoch 17 Batch 23 loss 0.6728959679603577\n",
            "Epoch 17 Batch 24 loss 0.7032436728477478\n",
            "Epoch 17 Batch 25 loss 0.6838628053665161\n",
            "Epoch 17 Batch 26 loss 0.6862480044364929\n",
            "Epoch 17 Batch 27 loss 0.6721187233924866\n",
            "Epoch 17 Batch 28 loss 0.6845443844795227\n",
            "Epoch 17 Batch 29 loss 0.6931453943252563\n",
            "Epoch 17 Batch 30 loss 0.6661074757575989\n",
            "Epoch 17 Batch 31 loss 0.6896114945411682\n",
            "Epoch 17 Batch 32 loss 0.6897121071815491\n",
            "Epoch 17 Batch 33 loss 0.6837707757949829\n",
            "Epoch 17 Batch 34 loss 0.664659321308136\n",
            "Epoch 17 Batch 35 loss 0.7013639211654663\n",
            "Epoch 17 Batch 36 loss 0.6828610301017761\n",
            "Epoch 17 Batch 37 loss 0.6876718997955322\n",
            "Epoch 17 Batch 38 loss 0.7067257761955261\n",
            "Epoch 17 Batch 39 loss 0.6888831853866577\n",
            "Epoch 17 Batch 40 loss 0.7130733132362366\n",
            "Epoch 17 Batch 41 loss 0.6706678867340088\n",
            "Epoch 17 Batch 42 loss 0.6986733675003052\n",
            "Epoch 17 Batch 43 loss 0.6878404021263123\n",
            "Epoch 17 Batch 44 loss 0.6753832697868347\n",
            "Epoch 17 Batch 45 loss 0.7081692218780518\n",
            "Epoch 17 Batch 46 loss 0.7005518674850464\n",
            "Epoch 17 Batch 47 loss 0.7119397521018982\n",
            "Epoch 17 Batch 48 loss 0.6652456521987915\n",
            "Epoch 17 Batch 49 loss 0.6948875784873962\n",
            "Epoch 17 Batch 50 loss 0.7187588214874268\n",
            "Epoch 17 Batch 51 loss 0.7057470083236694\n",
            "Epoch 17 Batch 52 loss 0.6874518990516663\n",
            "Epoch 17 Batch 53 loss 0.7240411639213562\n",
            "Epoch 17 Batch 54 loss 0.684873640537262\n",
            "Epoch 17 Batch 55 loss 0.6779709458351135\n",
            "Epoch 17 Batch 56 loss 0.6926533579826355\n",
            "Epoch 17 Batch 57 loss 0.7082057595252991\n",
            "Epoch 17 Batch 58 loss 0.7075738310813904\n",
            "Epoch 17 Batch 59 loss 0.7149406671524048\n",
            "Epoch 17 Batch 60 loss 0.7356683611869812\n",
            "Epoch 17 Batch 61 loss 0.7271278500556946\n",
            "Epoch 17 Batch 62 loss 0.7023117542266846\n",
            "Epoch 17 Batch 63 loss 0.7110458612442017\n",
            "Epoch 17 Batch 64 loss 0.7093494534492493\n",
            "Epoch 17 Batch 65 loss 0.7079871892929077\n",
            "Epoch 17 Batch 66 loss 0.7051330804824829\n",
            "Epoch 17 Batch 67 loss 0.6898361444473267\n",
            "Epoch 17 Batch 68 loss 0.7203659415245056\n",
            "Epoch 17 Batch 69 loss 0.7043898105621338\n",
            "Epoch 17 Batch 70 loss 0.6870301961898804\n",
            "Epoch 17 Batch 71 loss 0.7146707773208618\n",
            "Epoch 17 Batch 72 loss 0.7070470452308655\n",
            "Epoch 17 Batch 73 loss 0.7118003964424133\n",
            "Epoch 17 Batch 74 loss 0.7111403346061707\n",
            "Epoch 17 Batch 75 loss 0.723115861415863\n",
            "Epoch 17 Batch 76 loss 0.7041293978691101\n",
            "Epoch 17 Batch 77 loss 0.710503339767456\n",
            "Epoch 17 Batch 78 loss 0.7252863645553589\n",
            "Epoch 17 Batch 79 loss 0.7448118329048157\n",
            "Epoch 17 Batch 80 loss 0.722487211227417\n",
            "Epoch 17 Batch 81 loss 0.70900958776474\n",
            "Epoch 17 Batch 82 loss 0.7117753624916077\n",
            "Epoch 17 Batch 83 loss 0.69938725233078\n",
            "Epoch 17 Batch 84 loss 0.7178670763969421\n",
            "Epoch 17 Batch 85 loss 0.6922081708908081\n",
            "Epoch 17 Batch 86 loss 0.7328585982322693\n",
            "Epoch 17 Batch 87 loss 0.6981983780860901\n",
            "Epoch 17 Batch 88 loss 0.7072712779045105\n",
            "Epoch 17 Batch 89 loss 0.7300803065299988\n",
            "Epoch 17 Batch 90 loss 0.7015764117240906\n",
            "Epoch 17 Batch 91 loss 0.7090045809745789\n",
            "Epoch 17 Batch 92 loss 0.7031296491622925\n",
            "Epoch 17 Loss 0.6935\n",
            "Time taken for 1 epoch 122.2801685333252 sec\n",
            "\n",
            "Epoch 18 Batch 0 loss 0.5958561897277832\n",
            "Epoch 18 Batch 1 loss 0.6024721264839172\n",
            "Epoch 18 Batch 2 loss 0.5913209915161133\n",
            "Epoch 18 Batch 3 loss 0.640788733959198\n",
            "Epoch 18 Batch 4 loss 0.584566593170166\n",
            "Epoch 18 Batch 5 loss 0.6149274706840515\n",
            "Epoch 18 Batch 6 loss 0.6019420623779297\n",
            "Epoch 18 Batch 7 loss 0.5877060890197754\n",
            "Epoch 18 Batch 8 loss 0.5758183002471924\n",
            "Epoch 18 Batch 9 loss 0.576800525188446\n",
            "Epoch 18 Batch 10 loss 0.6270791292190552\n",
            "Epoch 18 Batch 11 loss 0.6088967323303223\n",
            "Epoch 18 Batch 12 loss 0.6121302843093872\n",
            "Epoch 18 Batch 13 loss 0.5974105000495911\n",
            "Epoch 18 Batch 14 loss 0.6436999440193176\n",
            "Epoch 18 Batch 15 loss 0.6127681136131287\n",
            "Epoch 18 Batch 16 loss 0.6029795408248901\n",
            "Epoch 18 Batch 17 loss 0.6019691824913025\n",
            "Epoch 18 Batch 18 loss 0.6212490200996399\n",
            "Epoch 18 Batch 19 loss 0.6346999406814575\n",
            "Epoch 18 Batch 20 loss 0.635149359703064\n",
            "Epoch 18 Batch 21 loss 0.6210722327232361\n",
            "Epoch 18 Batch 22 loss 0.6113235354423523\n",
            "Epoch 18 Batch 23 loss 0.6474587917327881\n",
            "Epoch 18 Batch 24 loss 0.6152666211128235\n",
            "Epoch 18 Batch 25 loss 0.6398496627807617\n",
            "Epoch 18 Batch 26 loss 0.6344302892684937\n",
            "Epoch 18 Batch 27 loss 0.633389413356781\n",
            "Epoch 18 Batch 28 loss 0.6143502593040466\n",
            "Epoch 18 Batch 29 loss 0.633171558380127\n",
            "Epoch 18 Batch 30 loss 0.631066083908081\n",
            "Epoch 18 Batch 31 loss 0.6300904750823975\n",
            "Epoch 18 Batch 32 loss 0.6201152205467224\n",
            "Epoch 18 Batch 33 loss 0.621817946434021\n",
            "Epoch 18 Batch 34 loss 0.6270832419395447\n",
            "Epoch 18 Batch 35 loss 0.6090707182884216\n",
            "Epoch 18 Batch 36 loss 0.6330612897872925\n",
            "Epoch 18 Batch 37 loss 0.6283268928527832\n",
            "Epoch 18 Batch 38 loss 0.5926958322525024\n",
            "Epoch 18 Batch 39 loss 0.6170791983604431\n",
            "Epoch 18 Batch 40 loss 0.6060329675674438\n",
            "Epoch 18 Batch 41 loss 0.6050187349319458\n",
            "Epoch 18 Batch 42 loss 0.6234220266342163\n",
            "Epoch 18 Batch 43 loss 0.6292529106140137\n",
            "Epoch 18 Batch 44 loss 0.6268310546875\n",
            "Epoch 18 Batch 45 loss 0.6351785063743591\n",
            "Epoch 18 Batch 46 loss 0.6278757452964783\n",
            "Epoch 18 Batch 47 loss 0.6142508387565613\n",
            "Epoch 18 Batch 48 loss 0.6207547187805176\n",
            "Epoch 18 Batch 49 loss 0.6284806728363037\n",
            "Epoch 18 Batch 50 loss 0.6649559140205383\n",
            "Epoch 18 Batch 51 loss 0.6123466491699219\n",
            "Epoch 18 Batch 52 loss 0.6462909579277039\n",
            "Epoch 18 Batch 53 loss 0.6525541543960571\n",
            "Epoch 18 Batch 54 loss 0.6252113580703735\n",
            "Epoch 18 Batch 55 loss 0.6532527804374695\n",
            "Epoch 18 Batch 56 loss 0.6183093786239624\n",
            "Epoch 18 Batch 57 loss 0.6374235153198242\n",
            "Epoch 18 Batch 58 loss 0.62447589635849\n",
            "Epoch 18 Batch 59 loss 0.6120785474777222\n",
            "Epoch 18 Batch 60 loss 0.6549748182296753\n",
            "Epoch 18 Batch 61 loss 0.6540625691413879\n",
            "Epoch 18 Batch 62 loss 0.6238042712211609\n",
            "Epoch 18 Batch 63 loss 0.6456822752952576\n",
            "Epoch 18 Batch 64 loss 0.6568936109542847\n",
            "Epoch 18 Batch 65 loss 0.6579007506370544\n",
            "Epoch 18 Batch 66 loss 0.6411966681480408\n",
            "Epoch 18 Batch 67 loss 0.67472243309021\n",
            "Epoch 18 Batch 68 loss 0.649522602558136\n",
            "Epoch 18 Batch 69 loss 0.6582491397857666\n",
            "Epoch 18 Batch 70 loss 0.6584095358848572\n",
            "Epoch 18 Batch 71 loss 0.6500704884529114\n",
            "Epoch 18 Batch 72 loss 0.6400351524353027\n",
            "Epoch 18 Batch 73 loss 0.6232231855392456\n",
            "Epoch 18 Batch 74 loss 0.625569224357605\n",
            "Epoch 18 Batch 75 loss 0.663085401058197\n",
            "Epoch 18 Batch 76 loss 0.6366669535636902\n",
            "Epoch 18 Batch 77 loss 0.640123724937439\n",
            "Epoch 18 Batch 78 loss 0.6387525796890259\n",
            "Epoch 18 Batch 79 loss 0.6271638870239258\n",
            "Epoch 18 Batch 80 loss 0.6448919177055359\n",
            "Epoch 18 Batch 81 loss 0.6427902579307556\n",
            "Epoch 18 Batch 82 loss 0.6457523107528687\n",
            "Epoch 18 Batch 83 loss 0.6477018594741821\n",
            "Epoch 18 Batch 84 loss 0.6684014201164246\n",
            "Epoch 18 Batch 85 loss 0.6386675238609314\n",
            "Epoch 18 Batch 86 loss 0.61717289686203\n",
            "Epoch 18 Batch 87 loss 0.6484860777854919\n",
            "Epoch 18 Batch 88 loss 0.6426852941513062\n",
            "Epoch 18 Batch 89 loss 0.6335206031799316\n",
            "Epoch 18 Batch 90 loss 0.6411069631576538\n",
            "Epoch 18 Batch 91 loss 0.6492512822151184\n",
            "Epoch 18 Batch 92 loss 0.6282278895378113\n",
            "Epoch 18 Loss 0.6286\n",
            "Time taken for 1 epoch 124.43897819519043 sec\n",
            "\n",
            "Epoch 19 Batch 0 loss 0.5470045804977417\n",
            "Epoch 19 Batch 1 loss 0.5473727583885193\n",
            "Epoch 19 Batch 2 loss 0.5489906072616577\n",
            "Epoch 19 Batch 3 loss 0.5316261053085327\n",
            "Epoch 19 Batch 4 loss 0.5374338030815125\n",
            "Epoch 19 Batch 5 loss 0.5324788093566895\n",
            "Epoch 19 Batch 6 loss 0.5494591593742371\n",
            "Epoch 19 Batch 7 loss 0.5551732182502747\n",
            "Epoch 19 Batch 8 loss 0.5439779162406921\n",
            "Epoch 19 Batch 9 loss 0.5381752252578735\n",
            "Epoch 19 Batch 10 loss 0.5624854564666748\n",
            "Epoch 19 Batch 11 loss 0.555941641330719\n",
            "Epoch 19 Batch 12 loss 0.5506686568260193\n",
            "Epoch 19 Batch 13 loss 0.5448826551437378\n",
            "Epoch 19 Batch 14 loss 0.5474850535392761\n",
            "Epoch 19 Batch 15 loss 0.5465553998947144\n",
            "Epoch 19 Batch 16 loss 0.5641335248947144\n",
            "Epoch 19 Batch 17 loss 0.526389479637146\n",
            "Epoch 19 Batch 18 loss 0.5316935777664185\n",
            "Epoch 19 Batch 19 loss 0.5530844926834106\n",
            "Epoch 19 Batch 20 loss 0.5504576563835144\n",
            "Epoch 19 Batch 21 loss 0.5393911004066467\n",
            "Epoch 19 Batch 22 loss 0.5443539023399353\n",
            "Epoch 19 Batch 23 loss 0.5471634268760681\n",
            "Epoch 19 Batch 24 loss 0.5638675093650818\n",
            "Epoch 19 Batch 25 loss 0.542902410030365\n",
            "Epoch 19 Batch 26 loss 0.5372004508972168\n",
            "Epoch 19 Batch 27 loss 0.5787181258201599\n",
            "Epoch 19 Batch 28 loss 0.5452722311019897\n",
            "Epoch 19 Batch 29 loss 0.573203444480896\n",
            "Epoch 19 Batch 30 loss 0.54283207654953\n",
            "Epoch 19 Batch 31 loss 0.5841782689094543\n",
            "Epoch 19 Batch 32 loss 0.5775025486946106\n",
            "Epoch 19 Batch 33 loss 0.5506097078323364\n",
            "Epoch 19 Batch 34 loss 0.5699617862701416\n",
            "Epoch 19 Batch 35 loss 0.5644943714141846\n",
            "Epoch 19 Batch 36 loss 0.5553213953971863\n",
            "Epoch 19 Batch 37 loss 0.559688925743103\n",
            "Epoch 19 Batch 38 loss 0.5662846565246582\n",
            "Epoch 19 Batch 39 loss 0.5680493116378784\n",
            "Epoch 19 Batch 40 loss 0.5551618933677673\n",
            "Epoch 19 Batch 41 loss 0.5661511421203613\n",
            "Epoch 19 Batch 42 loss 0.5584859251976013\n",
            "Epoch 19 Batch 43 loss 0.5905847549438477\n",
            "Epoch 19 Batch 44 loss 0.569516658782959\n",
            "Epoch 19 Batch 45 loss 0.5547168254852295\n",
            "Epoch 19 Batch 46 loss 0.596621036529541\n",
            "Epoch 19 Batch 47 loss 0.5847095847129822\n",
            "Epoch 19 Batch 48 loss 0.5747356414794922\n",
            "Epoch 19 Batch 49 loss 0.5815540552139282\n",
            "Epoch 19 Batch 50 loss 0.5716251134872437\n",
            "Epoch 19 Batch 51 loss 0.5835239887237549\n",
            "Epoch 19 Batch 52 loss 0.5751628279685974\n",
            "Epoch 19 Batch 53 loss 0.5733708739280701\n",
            "Epoch 19 Batch 54 loss 0.5902237892150879\n",
            "Epoch 19 Batch 55 loss 0.590168297290802\n",
            "Epoch 19 Batch 56 loss 0.5654477477073669\n",
            "Epoch 19 Batch 57 loss 0.5595053434371948\n",
            "Epoch 19 Batch 58 loss 0.5783042907714844\n",
            "Epoch 19 Batch 59 loss 0.5806992053985596\n",
            "Epoch 19 Batch 60 loss 0.5594011545181274\n",
            "Epoch 19 Batch 61 loss 0.5949043035507202\n",
            "Epoch 19 Batch 62 loss 0.5806443691253662\n",
            "Epoch 19 Batch 63 loss 0.5841435790061951\n",
            "Epoch 19 Batch 64 loss 0.5970067381858826\n",
            "Epoch 19 Batch 65 loss 0.5958890318870544\n",
            "Epoch 19 Batch 66 loss 0.5943209528923035\n",
            "Epoch 19 Batch 67 loss 0.5949122309684753\n",
            "Epoch 19 Batch 68 loss 0.5868873596191406\n",
            "Epoch 19 Batch 69 loss 0.5748788118362427\n",
            "Epoch 19 Batch 70 loss 0.5723794102668762\n",
            "Epoch 19 Batch 71 loss 0.5941374897956848\n",
            "Epoch 19 Batch 72 loss 0.6027030944824219\n",
            "Epoch 19 Batch 73 loss 0.5883814096450806\n",
            "Epoch 19 Batch 74 loss 0.56984543800354\n",
            "Epoch 19 Batch 75 loss 0.5853555798530579\n",
            "Epoch 19 Batch 76 loss 0.5946687459945679\n",
            "Epoch 19 Batch 77 loss 0.601920485496521\n",
            "Epoch 19 Batch 78 loss 0.6024531722068787\n",
            "Epoch 19 Batch 79 loss 0.601269543170929\n",
            "Epoch 19 Batch 80 loss 0.5752609968185425\n",
            "Epoch 19 Batch 81 loss 0.5893368721008301\n",
            "Epoch 19 Batch 82 loss 0.5975873470306396\n",
            "Epoch 19 Batch 83 loss 0.580051600933075\n",
            "Epoch 19 Batch 84 loss 0.6011986136436462\n",
            "Epoch 19 Batch 85 loss 0.6054172515869141\n",
            "Epoch 19 Batch 86 loss 0.5802793502807617\n",
            "Epoch 19 Batch 87 loss 0.5957849025726318\n",
            "Epoch 19 Batch 88 loss 0.5817722678184509\n",
            "Epoch 19 Batch 89 loss 0.5978078246116638\n",
            "Epoch 19 Batch 90 loss 0.6018669605255127\n",
            "Epoch 19 Batch 91 loss 0.6001672148704529\n",
            "Epoch 19 Batch 92 loss 0.6102005243301392\n",
            "Epoch 19 Loss 0.5706\n",
            "Time taken for 1 epoch 122.4908299446106 sec\n",
            "\n",
            "Epoch 20 Batch 0 loss 0.47882118821144104\n",
            "Epoch 20 Batch 1 loss 0.5029204487800598\n",
            "Epoch 20 Batch 2 loss 0.49596771597862244\n",
            "Epoch 20 Batch 3 loss 0.4716084897518158\n",
            "Epoch 20 Batch 4 loss 0.5015074014663696\n",
            "Epoch 20 Batch 5 loss 0.4731091558933258\n",
            "Epoch 20 Batch 6 loss 0.48547181487083435\n",
            "Epoch 20 Batch 7 loss 0.4873143434524536\n",
            "Epoch 20 Batch 8 loss 0.4991447925567627\n",
            "Epoch 20 Batch 9 loss 0.48639029264450073\n",
            "Epoch 20 Batch 10 loss 0.46563655138015747\n",
            "Epoch 20 Batch 11 loss 0.4677176773548126\n",
            "Epoch 20 Batch 12 loss 0.4927196800708771\n",
            "Epoch 20 Batch 13 loss 0.48274174332618713\n",
            "Epoch 20 Batch 14 loss 0.5009284615516663\n",
            "Epoch 20 Batch 15 loss 0.5028481483459473\n",
            "Epoch 20 Batch 16 loss 0.5083546042442322\n",
            "Epoch 20 Batch 17 loss 0.48491552472114563\n",
            "Epoch 20 Batch 18 loss 0.5135985016822815\n",
            "Epoch 20 Batch 19 loss 0.4893963932991028\n",
            "Epoch 20 Batch 20 loss 0.49337735772132874\n",
            "Epoch 20 Batch 21 loss 0.5156001448631287\n",
            "Epoch 20 Batch 22 loss 0.5167079567909241\n",
            "Epoch 20 Batch 23 loss 0.48782238364219666\n",
            "Epoch 20 Batch 24 loss 0.5045587420463562\n",
            "Epoch 20 Batch 25 loss 0.48820847272872925\n",
            "Epoch 20 Batch 26 loss 0.5017054080963135\n",
            "Epoch 20 Batch 27 loss 0.5075790286064148\n",
            "Epoch 20 Batch 28 loss 0.5235629677772522\n",
            "Epoch 20 Batch 29 loss 0.5058670043945312\n",
            "Epoch 20 Batch 30 loss 0.523048996925354\n",
            "Epoch 20 Batch 31 loss 0.5012059807777405\n",
            "Epoch 20 Batch 32 loss 0.5151589512825012\n",
            "Epoch 20 Batch 33 loss 0.526240348815918\n",
            "Epoch 20 Batch 34 loss 0.5190123915672302\n",
            "Epoch 20 Batch 35 loss 0.5027351379394531\n",
            "Epoch 20 Batch 36 loss 0.4937891960144043\n",
            "Epoch 20 Batch 37 loss 0.5394705533981323\n",
            "Epoch 20 Batch 38 loss 0.5221155881881714\n",
            "Epoch 20 Batch 39 loss 0.5084050297737122\n",
            "Epoch 20 Batch 40 loss 0.5318166613578796\n",
            "Epoch 20 Batch 41 loss 0.5113446116447449\n",
            "Epoch 20 Batch 42 loss 0.5263156890869141\n",
            "Epoch 20 Batch 43 loss 0.5243843793869019\n",
            "Epoch 20 Batch 44 loss 0.5181423425674438\n",
            "Epoch 20 Batch 45 loss 0.5090053677558899\n",
            "Epoch 20 Batch 46 loss 0.49519193172454834\n",
            "Epoch 20 Batch 47 loss 0.5137465000152588\n",
            "Epoch 20 Batch 48 loss 0.5171944499015808\n",
            "Epoch 20 Batch 49 loss 0.5475889444351196\n",
            "Epoch 20 Batch 50 loss 0.5289131999015808\n",
            "Epoch 20 Batch 51 loss 0.5198850035667419\n",
            "Epoch 20 Batch 52 loss 0.514388918876648\n",
            "Epoch 20 Batch 53 loss 0.5365461707115173\n",
            "Epoch 20 Batch 54 loss 0.5177217721939087\n",
            "Epoch 20 Batch 55 loss 0.5300055146217346\n",
            "Epoch 20 Batch 56 loss 0.5435068607330322\n",
            "Epoch 20 Batch 57 loss 0.5358065366744995\n",
            "Epoch 20 Batch 58 loss 0.5149216651916504\n",
            "Epoch 20 Batch 59 loss 0.5057950615882874\n",
            "Epoch 20 Batch 60 loss 0.5281454920768738\n",
            "Epoch 20 Batch 61 loss 0.507654070854187\n",
            "Epoch 20 Batch 62 loss 0.512565553188324\n",
            "Epoch 20 Batch 63 loss 0.5313010811805725\n",
            "Epoch 20 Batch 64 loss 0.5263308882713318\n",
            "Epoch 20 Batch 65 loss 0.5093479752540588\n",
            "Epoch 20 Batch 66 loss 0.5446664690971375\n",
            "Epoch 20 Batch 67 loss 0.5259789228439331\n",
            "Epoch 20 Batch 68 loss 0.5332620143890381\n",
            "Epoch 20 Batch 69 loss 0.5301453471183777\n",
            "Epoch 20 Batch 70 loss 0.5342656373977661\n",
            "Epoch 20 Batch 71 loss 0.5097205638885498\n",
            "Epoch 20 Batch 72 loss 0.5346905589103699\n",
            "Epoch 20 Batch 73 loss 0.5494919419288635\n",
            "Epoch 20 Batch 74 loss 0.5447667837142944\n",
            "Epoch 20 Batch 75 loss 0.5639707446098328\n",
            "Epoch 20 Batch 76 loss 0.5110749006271362\n",
            "Epoch 20 Batch 77 loss 0.5217410326004028\n",
            "Epoch 20 Batch 78 loss 0.5395116806030273\n",
            "Epoch 20 Batch 79 loss 0.523052453994751\n",
            "Epoch 20 Batch 80 loss 0.5555943846702576\n",
            "Epoch 20 Batch 81 loss 0.5073125958442688\n",
            "Epoch 20 Batch 82 loss 0.5463448762893677\n",
            "Epoch 20 Batch 83 loss 0.5314397811889648\n",
            "Epoch 20 Batch 84 loss 0.5288098454475403\n",
            "Epoch 20 Batch 85 loss 0.5556991100311279\n",
            "Epoch 20 Batch 86 loss 0.5393134355545044\n",
            "Epoch 20 Batch 87 loss 0.541385293006897\n",
            "Epoch 20 Batch 88 loss 0.5150803327560425\n",
            "Epoch 20 Batch 89 loss 0.5220407247543335\n",
            "Epoch 20 Batch 90 loss 0.5215728878974915\n",
            "Epoch 20 Batch 91 loss 0.5262959003448486\n",
            "Epoch 20 Batch 92 loss 0.5435389876365662\n",
            "Epoch 20 Loss 0.5155\n",
            "Time taken for 1 epoch 124.38442087173462 sec\n",
            "\n",
            "Epoch 21 Batch 0 loss 0.44961124658584595\n",
            "Epoch 21 Batch 1 loss 0.42830008268356323\n",
            "Epoch 21 Batch 2 loss 0.43414467573165894\n",
            "Epoch 21 Batch 3 loss 0.4388244152069092\n",
            "Epoch 21 Batch 4 loss 0.43078020215034485\n",
            "Epoch 21 Batch 5 loss 0.4270821511745453\n",
            "Epoch 21 Batch 6 loss 0.44688066840171814\n",
            "Epoch 21 Batch 7 loss 0.4400945007801056\n",
            "Epoch 21 Batch 8 loss 0.4452057182788849\n",
            "Epoch 21 Batch 9 loss 0.45063674449920654\n",
            "Epoch 21 Batch 10 loss 0.4224048852920532\n",
            "Epoch 21 Batch 11 loss 0.43907561898231506\n",
            "Epoch 21 Batch 12 loss 0.44013333320617676\n",
            "Epoch 21 Batch 13 loss 0.46492213010787964\n",
            "Epoch 21 Batch 14 loss 0.43848150968551636\n",
            "Epoch 21 Batch 15 loss 0.470458060503006\n",
            "Epoch 21 Batch 16 loss 0.4617178440093994\n",
            "Epoch 21 Batch 17 loss 0.4402506351470947\n",
            "Epoch 21 Batch 18 loss 0.4265071749687195\n",
            "Epoch 21 Batch 19 loss 0.4333866238594055\n",
            "Epoch 21 Batch 20 loss 0.46159571409225464\n",
            "Epoch 21 Batch 21 loss 0.4610009789466858\n",
            "Epoch 21 Batch 22 loss 0.45256808400154114\n",
            "Epoch 21 Batch 23 loss 0.4483034610748291\n",
            "Epoch 21 Batch 24 loss 0.46969377994537354\n",
            "Epoch 21 Batch 25 loss 0.45724406838417053\n",
            "Epoch 21 Batch 26 loss 0.4479040205478668\n",
            "Epoch 21 Batch 27 loss 0.45642217993736267\n",
            "Epoch 21 Batch 28 loss 0.4464460611343384\n",
            "Epoch 21 Batch 29 loss 0.4815850853919983\n",
            "Epoch 21 Batch 30 loss 0.4535417854785919\n",
            "Epoch 21 Batch 31 loss 0.4533984065055847\n",
            "Epoch 21 Batch 32 loss 0.45676541328430176\n",
            "Epoch 21 Batch 33 loss 0.46638691425323486\n",
            "Epoch 21 Batch 34 loss 0.47156962752342224\n",
            "Epoch 21 Batch 35 loss 0.4566167891025543\n",
            "Epoch 21 Batch 36 loss 0.48345789313316345\n",
            "Epoch 21 Batch 37 loss 0.45964938402175903\n",
            "Epoch 21 Batch 38 loss 0.4649222493171692\n",
            "Epoch 21 Batch 39 loss 0.4836582541465759\n",
            "Epoch 21 Batch 40 loss 0.4557528495788574\n",
            "Epoch 21 Batch 41 loss 0.482744038105011\n",
            "Epoch 21 Batch 42 loss 0.46413516998291016\n",
            "Epoch 21 Batch 43 loss 0.45711591839790344\n",
            "Epoch 21 Batch 44 loss 0.4574939012527466\n",
            "Epoch 21 Batch 45 loss 0.48276951909065247\n",
            "Epoch 21 Batch 46 loss 0.4804335832595825\n",
            "Epoch 21 Batch 47 loss 0.47509562969207764\n",
            "Epoch 21 Batch 48 loss 0.4710507392883301\n",
            "Epoch 21 Batch 49 loss 0.4742938280105591\n",
            "Epoch 21 Batch 50 loss 0.493074893951416\n",
            "Epoch 21 Batch 51 loss 0.4704188406467438\n",
            "Epoch 21 Batch 52 loss 0.4709053635597229\n",
            "Epoch 21 Batch 53 loss 0.4675079882144928\n",
            "Epoch 21 Batch 54 loss 0.48805147409439087\n",
            "Epoch 21 Batch 55 loss 0.5205987691879272\n",
            "Epoch 21 Batch 56 loss 0.5047113299369812\n",
            "Epoch 21 Batch 57 loss 0.5213205218315125\n",
            "Epoch 21 Batch 58 loss 0.5188554525375366\n",
            "Epoch 21 Batch 59 loss 0.5004541873931885\n",
            "Epoch 21 Batch 60 loss 0.4802124798297882\n",
            "Epoch 21 Batch 61 loss 0.5001678466796875\n",
            "Epoch 21 Batch 62 loss 0.5142410397529602\n",
            "Epoch 21 Batch 63 loss 0.500067412853241\n",
            "Epoch 21 Batch 64 loss 0.4872608184814453\n",
            "Epoch 21 Batch 65 loss 0.4988517761230469\n",
            "Epoch 21 Batch 66 loss 0.4712026119232178\n",
            "Epoch 21 Batch 67 loss 0.5060582756996155\n",
            "Epoch 21 Batch 68 loss 0.6708288788795471\n",
            "Epoch 21 Batch 69 loss 0.7300270199775696\n",
            "Epoch 21 Batch 70 loss 0.6384461522102356\n",
            "Epoch 21 Batch 71 loss 0.6772497892379761\n",
            "Epoch 21 Batch 72 loss 0.6940681338310242\n",
            "Epoch 21 Batch 73 loss 0.6995921730995178\n",
            "Epoch 21 Batch 74 loss 0.6682785153388977\n",
            "Epoch 21 Batch 75 loss 0.6762715578079224\n",
            "Epoch 21 Batch 76 loss 0.6356191039085388\n",
            "Epoch 21 Batch 77 loss 0.6599082350730896\n",
            "Epoch 21 Batch 78 loss 0.6275308132171631\n",
            "Epoch 21 Batch 79 loss 0.6441476345062256\n",
            "Epoch 21 Batch 80 loss 0.6102921962738037\n",
            "Epoch 21 Batch 81 loss 0.6054630279541016\n",
            "Epoch 21 Batch 82 loss 0.6066493988037109\n",
            "Epoch 21 Batch 83 loss 0.6137462854385376\n",
            "Epoch 21 Batch 84 loss 0.603447437286377\n",
            "Epoch 21 Batch 85 loss 0.6307749152183533\n",
            "Epoch 21 Batch 86 loss 0.6073436737060547\n",
            "Epoch 21 Batch 87 loss 0.5853902697563171\n",
            "Epoch 21 Batch 88 loss 0.5935533046722412\n",
            "Epoch 21 Batch 89 loss 0.6004431247711182\n",
            "Epoch 21 Batch 90 loss 0.5582460165023804\n",
            "Epoch 21 Batch 91 loss 0.5965316295623779\n",
            "Epoch 21 Batch 92 loss 0.5889124870300293\n",
            "Epoch 21 Loss 0.5107\n",
            "Time taken for 1 epoch 122.17326784133911 sec\n",
            "\n",
            "Epoch 22 Batch 0 loss 0.5112662315368652\n",
            "Epoch 22 Batch 1 loss 0.46804457902908325\n",
            "Epoch 22 Batch 2 loss 0.46023499965667725\n",
            "Epoch 22 Batch 3 loss 0.49870097637176514\n",
            "Epoch 22 Batch 4 loss 0.46587228775024414\n",
            "Epoch 22 Batch 5 loss 0.4804535508155823\n",
            "Epoch 22 Batch 6 loss 0.47385263442993164\n",
            "Epoch 22 Batch 7 loss 0.47552135586738586\n",
            "Epoch 22 Batch 8 loss 0.47988730669021606\n",
            "Epoch 22 Batch 9 loss 0.4699092209339142\n",
            "Epoch 22 Batch 10 loss 0.4764757752418518\n",
            "Epoch 22 Batch 11 loss 0.46635931730270386\n",
            "Epoch 22 Batch 12 loss 0.47704756259918213\n",
            "Epoch 22 Batch 13 loss 0.47165602445602417\n",
            "Epoch 22 Batch 14 loss 0.4666951894760132\n",
            "Epoch 22 Batch 15 loss 0.48162785172462463\n",
            "Epoch 22 Batch 16 loss 0.44962960481643677\n",
            "Epoch 22 Batch 17 loss 0.4790405333042145\n",
            "Epoch 22 Batch 18 loss 0.45598548650741577\n",
            "Epoch 22 Batch 19 loss 0.44652262330055237\n",
            "Epoch 22 Batch 20 loss 0.47098472714424133\n",
            "Epoch 22 Batch 21 loss 0.439162015914917\n",
            "Epoch 22 Batch 22 loss 0.4702562689781189\n",
            "Epoch 22 Batch 23 loss 0.48219171166419983\n",
            "Epoch 22 Batch 24 loss 0.46250107884407043\n",
            "Epoch 22 Batch 25 loss 0.44097456336021423\n",
            "Epoch 22 Batch 26 loss 0.4629576802253723\n",
            "Epoch 22 Batch 27 loss 0.4562973380088806\n",
            "Epoch 22 Batch 28 loss 0.43660929799079895\n",
            "Epoch 22 Batch 29 loss 0.46035516262054443\n",
            "Epoch 22 Batch 30 loss 0.43991389870643616\n",
            "Epoch 22 Batch 31 loss 0.4409026503562927\n",
            "Epoch 22 Batch 32 loss 0.4870125651359558\n",
            "Epoch 22 Batch 33 loss 0.4832310676574707\n",
            "Epoch 22 Batch 34 loss 0.4640365242958069\n",
            "Epoch 22 Batch 35 loss 0.47956541180610657\n",
            "Epoch 22 Batch 36 loss 0.46632540225982666\n",
            "Epoch 22 Batch 37 loss 0.5070756077766418\n",
            "Epoch 22 Batch 38 loss 0.45325618982315063\n",
            "Epoch 22 Batch 39 loss 0.4472392201423645\n",
            "Epoch 22 Batch 40 loss 0.4500766098499298\n",
            "Epoch 22 Batch 41 loss 0.46901607513427734\n",
            "Epoch 22 Batch 42 loss 0.46173810958862305\n",
            "Epoch 22 Batch 43 loss 0.44757771492004395\n",
            "Epoch 22 Batch 44 loss 0.45156407356262207\n",
            "Epoch 22 Batch 45 loss 0.44998303055763245\n",
            "Epoch 22 Batch 46 loss 0.44275012612342834\n",
            "Epoch 22 Batch 47 loss 0.45476865768432617\n",
            "Epoch 22 Batch 48 loss 0.4496699273586273\n",
            "Epoch 22 Batch 49 loss 0.4473879635334015\n",
            "Epoch 22 Batch 50 loss 0.43848979473114014\n",
            "Epoch 22 Batch 51 loss 0.47083884477615356\n",
            "Epoch 22 Batch 52 loss 0.45751291513442993\n",
            "Epoch 22 Batch 53 loss 0.45192751288414\n",
            "Epoch 22 Batch 54 loss 0.45133697986602783\n",
            "Epoch 22 Batch 55 loss 0.4720059335231781\n",
            "Epoch 22 Batch 56 loss 0.4564763307571411\n",
            "Epoch 22 Batch 57 loss 0.4619861841201782\n",
            "Epoch 22 Batch 58 loss 0.43853601813316345\n",
            "Epoch 22 Batch 59 loss 0.4425414204597473\n",
            "Epoch 22 Batch 60 loss 0.4491804838180542\n",
            "Epoch 22 Batch 61 loss 0.4475994408130646\n",
            "Epoch 22 Batch 62 loss 0.4554532766342163\n",
            "Epoch 22 Batch 63 loss 0.4511648714542389\n",
            "Epoch 22 Batch 64 loss 0.4505902826786041\n",
            "Epoch 22 Batch 65 loss 0.467606782913208\n",
            "Epoch 22 Batch 66 loss 0.4676402509212494\n",
            "Epoch 22 Batch 67 loss 0.4564078450202942\n",
            "Epoch 22 Batch 68 loss 0.46042361855506897\n",
            "Epoch 22 Batch 69 loss 0.4266013205051422\n",
            "Epoch 22 Batch 70 loss 0.4515233039855957\n",
            "Epoch 22 Batch 71 loss 0.4602563977241516\n",
            "Epoch 22 Batch 72 loss 0.46281901001930237\n",
            "Epoch 22 Batch 73 loss 0.47091594338417053\n",
            "Epoch 22 Batch 74 loss 0.4645778238773346\n",
            "Epoch 22 Batch 75 loss 0.4536280930042267\n",
            "Epoch 22 Batch 76 loss 0.45171844959259033\n",
            "Epoch 22 Batch 77 loss 0.45071202516555786\n",
            "Epoch 22 Batch 78 loss 0.4379943907260895\n",
            "Epoch 22 Batch 79 loss 0.4634549915790558\n",
            "Epoch 22 Batch 80 loss 0.4575798809528351\n",
            "Epoch 22 Batch 81 loss 0.4665928781032562\n",
            "Epoch 22 Batch 82 loss 0.46489375829696655\n",
            "Epoch 22 Batch 83 loss 0.4464210867881775\n",
            "Epoch 22 Batch 84 loss 0.4609699845314026\n",
            "Epoch 22 Batch 85 loss 0.4552205801010132\n",
            "Epoch 22 Batch 86 loss 0.4578215479850769\n",
            "Epoch 22 Batch 87 loss 0.4585501253604889\n",
            "Epoch 22 Batch 88 loss 0.46867406368255615\n",
            "Epoch 22 Batch 89 loss 0.4558127820491791\n",
            "Epoch 22 Batch 90 loss 0.45597222447395325\n",
            "Epoch 22 Batch 91 loss 0.4456779360771179\n",
            "Epoch 22 Batch 92 loss 0.4528356194496155\n",
            "Epoch 22 Loss 0.4604\n",
            "Time taken for 1 epoch 124.1660430431366 sec\n",
            "\n",
            "Epoch 23 Batch 0 loss 0.3694312274456024\n",
            "Epoch 23 Batch 1 loss 0.36936721205711365\n",
            "Epoch 23 Batch 2 loss 0.36369869112968445\n",
            "Epoch 23 Batch 3 loss 0.3878294825553894\n",
            "Epoch 23 Batch 4 loss 0.3592342138290405\n",
            "Epoch 23 Batch 5 loss 0.34887832403182983\n",
            "Epoch 23 Batch 6 loss 0.3945555090904236\n",
            "Epoch 23 Batch 7 loss 0.36716118454933167\n",
            "Epoch 23 Batch 8 loss 0.378206342458725\n",
            "Epoch 23 Batch 9 loss 0.39501455426216125\n",
            "Epoch 23 Batch 10 loss 0.3901180922985077\n",
            "Epoch 23 Batch 11 loss 0.36362752318382263\n",
            "Epoch 23 Batch 12 loss 0.36193493008613586\n",
            "Epoch 23 Batch 13 loss 0.39128220081329346\n",
            "Epoch 23 Batch 14 loss 0.36719244718551636\n",
            "Epoch 23 Batch 15 loss 0.37394264340400696\n",
            "Epoch 23 Batch 16 loss 0.37383052706718445\n",
            "Epoch 23 Batch 17 loss 0.37773481011390686\n",
            "Epoch 23 Batch 18 loss 0.37796345353126526\n",
            "Epoch 23 Batch 19 loss 0.40068545937538147\n",
            "Epoch 23 Batch 20 loss 0.3681759238243103\n",
            "Epoch 23 Batch 21 loss 0.3832748234272003\n",
            "Epoch 23 Batch 22 loss 0.3821152448654175\n",
            "Epoch 23 Batch 23 loss 0.3634251654148102\n",
            "Epoch 23 Batch 24 loss 0.40349534153938293\n",
            "Epoch 23 Batch 25 loss 0.38257133960723877\n",
            "Epoch 23 Batch 26 loss 0.38493362069129944\n",
            "Epoch 23 Batch 27 loss 0.37590134143829346\n",
            "Epoch 23 Batch 28 loss 0.3843211233615875\n",
            "Epoch 23 Batch 29 loss 0.3907178044319153\n",
            "Epoch 23 Batch 30 loss 0.39644214510917664\n",
            "Epoch 23 Batch 31 loss 0.3904590904712677\n",
            "Epoch 23 Batch 32 loss 0.39683592319488525\n",
            "Epoch 23 Batch 33 loss 0.38152220845222473\n",
            "Epoch 23 Batch 34 loss 0.40162521600723267\n",
            "Epoch 23 Batch 35 loss 0.39642661809921265\n",
            "Epoch 23 Batch 36 loss 0.3949309289455414\n",
            "Epoch 23 Batch 37 loss 0.39304524660110474\n",
            "Epoch 23 Batch 38 loss 0.4068205654621124\n",
            "Epoch 23 Batch 39 loss 0.3888374865055084\n",
            "Epoch 23 Batch 40 loss 0.3855864405632019\n",
            "Epoch 23 Batch 41 loss 0.37014663219451904\n",
            "Epoch 23 Batch 42 loss 0.39479121565818787\n",
            "Epoch 23 Batch 43 loss 0.38332217931747437\n",
            "Epoch 23 Batch 44 loss 0.3832945227622986\n",
            "Epoch 23 Batch 45 loss 0.3857037127017975\n",
            "Epoch 23 Batch 46 loss 0.38662078976631165\n",
            "Epoch 23 Batch 47 loss 0.37822115421295166\n",
            "Epoch 23 Batch 48 loss 0.40754759311676025\n",
            "Epoch 23 Batch 49 loss 0.39989909529685974\n",
            "Epoch 23 Batch 50 loss 0.40504178404808044\n",
            "Epoch 23 Batch 51 loss 0.39273878931999207\n",
            "Epoch 23 Batch 52 loss 0.4100722372531891\n",
            "Epoch 23 Batch 53 loss 0.4140641987323761\n",
            "Epoch 23 Batch 54 loss 0.4042067229747772\n",
            "Epoch 23 Batch 55 loss 0.38396456837654114\n",
            "Epoch 23 Batch 56 loss 0.3996071517467499\n",
            "Epoch 23 Batch 57 loss 0.39126551151275635\n",
            "Epoch 23 Batch 58 loss 0.384262353181839\n",
            "Epoch 23 Batch 59 loss 0.392296701669693\n",
            "Epoch 23 Batch 60 loss 0.40861448645591736\n",
            "Epoch 23 Batch 61 loss 0.39005839824676514\n",
            "Epoch 23 Batch 62 loss 0.4029233753681183\n",
            "Epoch 23 Batch 63 loss 0.39441820979118347\n",
            "Epoch 23 Batch 64 loss 0.40990251302719116\n",
            "Epoch 23 Batch 65 loss 0.38451582193374634\n",
            "Epoch 23 Batch 66 loss 0.39194169640541077\n",
            "Epoch 23 Batch 67 loss 0.39920079708099365\n",
            "Epoch 23 Batch 68 loss 0.4240056574344635\n",
            "Epoch 23 Batch 69 loss 0.4266059994697571\n",
            "Epoch 23 Batch 70 loss 0.38696762919425964\n",
            "Epoch 23 Batch 71 loss 0.40828531980514526\n",
            "Epoch 23 Batch 72 loss 0.39118751883506775\n",
            "Epoch 23 Batch 73 loss 0.39965158700942993\n",
            "Epoch 23 Batch 74 loss 0.39772042632102966\n",
            "Epoch 23 Batch 75 loss 0.4052935540676117\n",
            "Epoch 23 Batch 76 loss 0.41191089153289795\n",
            "Epoch 23 Batch 77 loss 0.4055907428264618\n",
            "Epoch 23 Batch 78 loss 0.4182130694389343\n",
            "Epoch 23 Batch 79 loss 0.40868452191352844\n",
            "Epoch 23 Batch 80 loss 0.4052826166152954\n",
            "Epoch 23 Batch 81 loss 0.40260061621665955\n",
            "Epoch 23 Batch 82 loss 0.4121941924095154\n",
            "Epoch 23 Batch 83 loss 0.38859447836875916\n",
            "Epoch 23 Batch 84 loss 0.4105755686759949\n",
            "Epoch 23 Batch 85 loss 0.4113816022872925\n",
            "Epoch 23 Batch 86 loss 0.3885941803455353\n",
            "Epoch 23 Batch 87 loss 0.3951778709888458\n",
            "Epoch 23 Batch 88 loss 0.41670238971710205\n",
            "Epoch 23 Batch 89 loss 0.4009973704814911\n",
            "Epoch 23 Batch 90 loss 0.42470523715019226\n",
            "Epoch 23 Batch 91 loss 0.4203595519065857\n",
            "Epoch 23 Batch 92 loss 0.40714266896247864\n",
            "Epoch 23 Loss 0.3923\n",
            "Time taken for 1 epoch 122.17572832107544 sec\n",
            "\n",
            "Epoch 24 Batch 0 loss 0.3437518775463104\n",
            "Epoch 24 Batch 1 loss 0.33190909028053284\n",
            "Epoch 24 Batch 2 loss 0.33534711599349976\n",
            "Epoch 24 Batch 3 loss 0.33257097005844116\n",
            "Epoch 24 Batch 4 loss 0.3229338824748993\n",
            "Epoch 24 Batch 5 loss 0.3437862694263458\n",
            "Epoch 24 Batch 6 loss 0.3170998990535736\n",
            "Epoch 24 Batch 7 loss 0.3367563486099243\n",
            "Epoch 24 Batch 8 loss 0.32867780327796936\n",
            "Epoch 24 Batch 9 loss 0.3461052179336548\n",
            "Epoch 24 Batch 10 loss 0.32663479447364807\n",
            "Epoch 24 Batch 11 loss 0.3268923759460449\n",
            "Epoch 24 Batch 12 loss 0.3487418591976166\n",
            "Epoch 24 Batch 13 loss 0.34987178444862366\n",
            "Epoch 24 Batch 14 loss 0.33670252561569214\n",
            "Epoch 24 Batch 15 loss 0.33660486340522766\n",
            "Epoch 24 Batch 16 loss 0.34036776423454285\n",
            "Epoch 24 Batch 17 loss 0.32548511028289795\n",
            "Epoch 24 Batch 18 loss 0.31979697942733765\n",
            "Epoch 24 Batch 19 loss 0.34949246048927307\n",
            "Epoch 24 Batch 20 loss 0.3447733521461487\n",
            "Epoch 24 Batch 21 loss 0.33341601490974426\n",
            "Epoch 24 Batch 22 loss 0.3320544362068176\n",
            "Epoch 24 Batch 23 loss 0.31959766149520874\n",
            "Epoch 24 Batch 24 loss 0.3316323459148407\n",
            "Epoch 24 Batch 25 loss 0.34889689087867737\n",
            "Epoch 24 Batch 26 loss 0.3472432494163513\n",
            "Epoch 24 Batch 27 loss 0.323081910610199\n",
            "Epoch 24 Batch 28 loss 0.35698044300079346\n",
            "Epoch 24 Batch 29 loss 0.34877726435661316\n",
            "Epoch 24 Batch 30 loss 0.32780954241752625\n",
            "Epoch 24 Batch 31 loss 0.32984086871147156\n",
            "Epoch 24 Batch 32 loss 0.3339311480522156\n",
            "Epoch 24 Batch 33 loss 0.3244139552116394\n",
            "Epoch 24 Batch 34 loss 0.3434843122959137\n",
            "Epoch 24 Batch 35 loss 0.3380512297153473\n",
            "Epoch 24 Batch 36 loss 0.34498366713523865\n",
            "Epoch 24 Batch 37 loss 0.32745352387428284\n",
            "Epoch 24 Batch 38 loss 0.3517512083053589\n",
            "Epoch 24 Batch 39 loss 0.34876149892807007\n",
            "Epoch 24 Batch 40 loss 0.3503042757511139\n",
            "Epoch 24 Batch 41 loss 0.3267967104911804\n",
            "Epoch 24 Batch 42 loss 0.3359071910381317\n",
            "Epoch 24 Batch 43 loss 0.3419882357120514\n",
            "Epoch 24 Batch 44 loss 0.3524150252342224\n",
            "Epoch 24 Batch 45 loss 0.32508718967437744\n",
            "Epoch 24 Batch 46 loss 0.3552674651145935\n",
            "Epoch 24 Batch 47 loss 0.37092480063438416\n",
            "Epoch 24 Batch 48 loss 0.36069172620773315\n",
            "Epoch 24 Batch 49 loss 0.3572615683078766\n",
            "Epoch 24 Batch 50 loss 0.36799493432044983\n",
            "Epoch 24 Batch 51 loss 0.35599249601364136\n",
            "Epoch 24 Batch 52 loss 0.34340742230415344\n",
            "Epoch 24 Batch 53 loss 0.35339051485061646\n",
            "Epoch 24 Batch 54 loss 0.36288270354270935\n",
            "Epoch 24 Batch 55 loss 0.35388508439064026\n",
            "Epoch 24 Batch 56 loss 0.35002586245536804\n",
            "Epoch 24 Batch 57 loss 0.3400322198867798\n",
            "Epoch 24 Batch 58 loss 0.35455426573753357\n",
            "Epoch 24 Batch 59 loss 0.36248910427093506\n",
            "Epoch 24 Batch 60 loss 0.3530419170856476\n",
            "Epoch 24 Batch 61 loss 0.3553834855556488\n",
            "Epoch 24 Batch 62 loss 0.3747987449169159\n",
            "Epoch 24 Batch 63 loss 0.358272910118103\n",
            "Epoch 24 Batch 64 loss 0.350686639547348\n",
            "Epoch 24 Batch 65 loss 0.3606390058994293\n",
            "Epoch 24 Batch 66 loss 0.3758434057235718\n",
            "Epoch 24 Batch 67 loss 0.3708948493003845\n",
            "Epoch 24 Batch 68 loss 0.36973562836647034\n",
            "Epoch 24 Batch 69 loss 0.37875816226005554\n",
            "Epoch 24 Batch 70 loss 0.3575685918331146\n",
            "Epoch 24 Batch 71 loss 0.3664945960044861\n",
            "Epoch 24 Batch 72 loss 0.3636360466480255\n",
            "Epoch 24 Batch 73 loss 0.3600257337093353\n",
            "Epoch 24 Batch 74 loss 0.3595801889896393\n",
            "Epoch 24 Batch 75 loss 0.36766940355300903\n",
            "Epoch 24 Batch 76 loss 0.3513299226760864\n",
            "Epoch 24 Batch 77 loss 0.35779622197151184\n",
            "Epoch 24 Batch 78 loss 0.3662759065628052\n",
            "Epoch 24 Batch 79 loss 0.35708102583885193\n",
            "Epoch 24 Batch 80 loss 0.3682367503643036\n",
            "Epoch 24 Batch 81 loss 0.36027610301971436\n",
            "Epoch 24 Batch 82 loss 0.33757254481315613\n",
            "Epoch 24 Batch 83 loss 0.37915125489234924\n",
            "Epoch 24 Batch 84 loss 0.3578571379184723\n",
            "Epoch 24 Batch 85 loss 0.3787556290626526\n",
            "Epoch 24 Batch 86 loss 0.3766306936740875\n",
            "Epoch 24 Batch 87 loss 0.3852331340312958\n",
            "Epoch 24 Batch 88 loss 0.36966070532798767\n",
            "Epoch 24 Batch 89 loss 0.39689603447914124\n",
            "Epoch 24 Batch 90 loss 0.35705894231796265\n",
            "Epoch 24 Batch 91 loss 0.36553820967674255\n",
            "Epoch 24 Batch 92 loss 0.36227336525917053\n",
            "Epoch 24 Loss 0.3498\n",
            "Time taken for 1 epoch 123.86206793785095 sec\n",
            "\n",
            "Epoch 25 Batch 0 loss 0.2890250086784363\n",
            "Epoch 25 Batch 1 loss 0.3202819526195526\n",
            "Epoch 25 Batch 2 loss 0.28852882981300354\n",
            "Epoch 25 Batch 3 loss 0.28885403275489807\n",
            "Epoch 25 Batch 4 loss 0.2873288094997406\n",
            "Epoch 25 Batch 5 loss 0.29980549216270447\n",
            "Epoch 25 Batch 6 loss 0.2881937623023987\n",
            "Epoch 25 Batch 7 loss 0.26915961503982544\n",
            "Epoch 25 Batch 8 loss 0.29058682918548584\n",
            "Epoch 25 Batch 9 loss 0.2892109751701355\n",
            "Epoch 25 Batch 10 loss 0.29740703105926514\n",
            "Epoch 25 Batch 11 loss 0.2907681465148926\n",
            "Epoch 25 Batch 12 loss 0.2968306541442871\n",
            "Epoch 25 Batch 13 loss 0.3216755986213684\n",
            "Epoch 25 Batch 14 loss 0.29905039072036743\n",
            "Epoch 25 Batch 15 loss 0.29932901263237\n",
            "Epoch 25 Batch 16 loss 0.3073312044143677\n",
            "Epoch 25 Batch 17 loss 0.29557153582572937\n",
            "Epoch 25 Batch 18 loss 0.3018719255924225\n",
            "Epoch 25 Batch 19 loss 0.2945234775543213\n",
            "Epoch 25 Batch 20 loss 0.30203986167907715\n",
            "Epoch 25 Batch 21 loss 0.29129594564437866\n",
            "Epoch 25 Batch 22 loss 0.3174842596054077\n",
            "Epoch 25 Batch 23 loss 0.3068126142024994\n",
            "Epoch 25 Batch 24 loss 0.2849777340888977\n",
            "Epoch 25 Batch 25 loss 0.30566349625587463\n",
            "Epoch 25 Batch 26 loss 0.300104558467865\n",
            "Epoch 25 Batch 27 loss 0.30767422914505005\n",
            "Epoch 25 Batch 28 loss 0.3120286166667938\n",
            "Epoch 25 Batch 29 loss 0.31918662786483765\n",
            "Epoch 25 Batch 30 loss 0.3212530314922333\n",
            "Epoch 25 Batch 31 loss 0.3333778977394104\n",
            "Epoch 25 Batch 32 loss 0.32554689049720764\n",
            "Epoch 25 Batch 33 loss 0.29896998405456543\n",
            "Epoch 25 Batch 34 loss 0.31615060567855835\n",
            "Epoch 25 Batch 35 loss 0.2996988296508789\n",
            "Epoch 25 Batch 36 loss 0.3082798421382904\n",
            "Epoch 25 Batch 37 loss 0.2971406877040863\n",
            "Epoch 25 Batch 38 loss 0.3094436228275299\n",
            "Epoch 25 Batch 39 loss 0.3393670618534088\n",
            "Epoch 25 Batch 40 loss 0.331946462392807\n",
            "Epoch 25 Batch 41 loss 0.3139124810695648\n",
            "Epoch 25 Batch 42 loss 0.31904396414756775\n",
            "Epoch 25 Batch 43 loss 0.2972279489040375\n",
            "Epoch 25 Batch 44 loss 0.3154255747795105\n",
            "Epoch 25 Batch 45 loss 0.3162236213684082\n",
            "Epoch 25 Batch 46 loss 0.32397109270095825\n",
            "Epoch 25 Batch 47 loss 0.31128230690956116\n",
            "Epoch 25 Batch 48 loss 0.3250144422054291\n",
            "Epoch 25 Batch 49 loss 0.3332034647464752\n",
            "Epoch 25 Batch 50 loss 0.3052539825439453\n",
            "Epoch 25 Batch 51 loss 0.3213130235671997\n",
            "Epoch 25 Batch 52 loss 0.3183680474758148\n",
            "Epoch 25 Batch 53 loss 0.3244909346103668\n",
            "Epoch 25 Batch 54 loss 0.3104361891746521\n",
            "Epoch 25 Batch 55 loss 0.3310631215572357\n",
            "Epoch 25 Batch 56 loss 0.3061198890209198\n",
            "Epoch 25 Batch 57 loss 0.32928940653800964\n",
            "Epoch 25 Batch 58 loss 0.32245102524757385\n",
            "Epoch 25 Batch 59 loss 0.3332369029521942\n",
            "Epoch 25 Batch 60 loss 0.32313066720962524\n",
            "Epoch 25 Batch 61 loss 0.31895002722740173\n",
            "Epoch 25 Batch 62 loss 0.3205386996269226\n",
            "Epoch 25 Batch 63 loss 0.3101778030395508\n",
            "Epoch 25 Batch 64 loss 0.3232570290565491\n",
            "Epoch 25 Batch 65 loss 0.350270539522171\n",
            "Epoch 25 Batch 66 loss 0.3205500543117523\n",
            "Epoch 25 Batch 67 loss 0.32777655124664307\n",
            "Epoch 25 Batch 68 loss 0.31761881709098816\n",
            "Epoch 25 Batch 69 loss 0.33168211579322815\n",
            "Epoch 25 Batch 70 loss 0.3176252543926239\n",
            "Epoch 25 Batch 71 loss 0.31794625520706177\n",
            "Epoch 25 Batch 72 loss 0.31753289699554443\n",
            "Epoch 25 Batch 73 loss 0.3279780149459839\n",
            "Epoch 25 Batch 74 loss 0.3321099877357483\n",
            "Epoch 25 Batch 75 loss 0.33237284421920776\n",
            "Epoch 25 Batch 76 loss 0.3302045464515686\n",
            "Epoch 25 Batch 77 loss 0.32534828782081604\n",
            "Epoch 25 Batch 78 loss 0.3379596173763275\n",
            "Epoch 25 Batch 79 loss 0.32888802886009216\n",
            "Epoch 25 Batch 80 loss 0.32881513237953186\n",
            "Epoch 25 Batch 81 loss 0.325554221868515\n",
            "Epoch 25 Batch 82 loss 0.31776949763298035\n",
            "Epoch 25 Batch 83 loss 0.32060980796813965\n",
            "Epoch 25 Batch 84 loss 0.30733323097229004\n",
            "Epoch 25 Batch 85 loss 0.3300914764404297\n",
            "Epoch 25 Batch 86 loss 0.3432324528694153\n",
            "Epoch 25 Batch 87 loss 0.32421737909317017\n",
            "Epoch 25 Batch 88 loss 0.31190621852874756\n",
            "Epoch 25 Batch 89 loss 0.3173242211341858\n",
            "Epoch 25 Batch 90 loss 0.3303757905960083\n",
            "Epoch 25 Batch 91 loss 0.31908199191093445\n",
            "Epoch 25 Batch 92 loss 0.3401605486869812\n",
            "Epoch 25 Loss 0.3143\n",
            "Time taken for 1 epoch 122.2441999912262 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7MpqSuVkOwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_target_length, max_source_length))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "  #print(sentence)\n",
        "  #print(source_sentence_tokenizer.word_index)\n",
        "\n",
        "  inputs = [source_sentence_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_source_length,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)\n",
        "\n",
        "  for t in range(max_target_length):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += target_sentence_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if target_sentence_tokenizer.index_word[predicted_id] == '_end':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVFLtD0RkRDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3BNnq3LkSOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "  \n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUMyovRJkTRB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ae242bb-6d36-41ec-9e55-e0511569c5a6"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f00747899e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjerGUR4kUiU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "9f51e256-8410-4ed5-84c1-53e3465b87d5"
      },
      "source": [
        "translate(u'I am going to school.')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ i am going to school . _end\n",
            "Predicted translation: ben okula gidiyorum . _end \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E1ixcHfkVtH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "9cd5f950-02e7-49cf-d72b-e7322f2f5bf0"
      },
      "source": [
        "translate(u'You need to go.')"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ you need to go . _end\n",
            "Predicted translation: gitmeniz gerekiyor . _end \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ7vGKhakXi7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "outputId": "17419062-fcd1-477c-843e-6442a60968e1"
      },
      "source": [
        "translate(u'I am a teacher.')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ i am a teacher . _end\n",
            "Predicted translation: ben bir i̇ngilizce ogretmeniyim . _end \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}